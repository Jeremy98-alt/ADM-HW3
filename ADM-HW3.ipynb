{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import lxml #is a parser for HTMl\n",
    "import webscraping\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "#from selenium import webdriver\n",
    "from langdetect import detect\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shoul generalize these functions... in webscraping.py!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **[RQ1.1] Get the list of books**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start to scrape the three pages to extract the URLs for each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "UrlsFiles = open(\"urlpages.txt\", \"w\")\n",
    "\n",
    "url = \"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"+str(1)\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, features='lxml')\n",
    "for a in soup.find_all('a', class_=\"bookTitle\"): #.contents[0].split('\\n')[1].strip()\n",
    "    UrlsFiles.write(a.get('href')+'\\n')\n",
    "\n",
    "#UrlsFiles.truncate(UrlsFiles.tell()-1) see how to remove the '\\n'\n",
    "UrlsFiles.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RQ[1.2] Crawl books**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "headpart = \"https://www.goodreads.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = \"htmlpages\" #Directory  \n",
    "parent_dir = \"./\" #Parent Directory path \n",
    "pathAncestor = os.path.join(parent_dir, direct) #Path\n",
    "os.mkdir(pathAncestor) #create the folder in the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,101):\n",
    "    os.makedirs(os.path.join(pathAncestor, 'page ' + str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "UrlsFiles = open(\"urlpages.txt\", \"r\")\n",
    "\n",
    "for i, x in enumerate(UrlsFiles,1):\n",
    "    subdirectory = pathAncestor + \"/page \" + str(i)\n",
    "    article_name = \"/article_\"+str(i)+\".html\"\n",
    "    \n",
    "    complete_path = subdirectory + article_name\n",
    "    with open(complete_path, \"wb\") as ip_file:\n",
    "        link = headpart + x\n",
    "        page = requests.get(link)\n",
    "        \n",
    "        soup = BeautifulSoup(page.text, features='lxml')\n",
    "        \n",
    "        ip_file.write(soup.encode('utf-8'))\n",
    "        ip_file.close()\n",
    "\n",
    "UrlsFiles.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RQ[1.3] Parse downloaded pages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_book(href, tsv_writer, Url):\n",
    "    r = requests.get(href) #call to have the html page with the useful information for my analysis\n",
    "    soup = BeautifulSoup(r.text, features=\"lxml\") #parse the text\n",
    "    \n",
    "    #extract rating and review count\n",
    "    ratings = soup.find_all('a', href=\"#other_reviews\") #search the ratings in its\n",
    "    rating_count = -1\n",
    "    rating = -1\n",
    "    for raiting in ratings:\n",
    "        if raiting.find_all('meta', itemprop=\"ratingCount\"):\n",
    "            ratingCount = raiting.text.replace('\\n', '').strip().split(' ')[0].replace(',', '')\n",
    "        elif raiting.find_all('meta', itemprop=\"reviewCount\"):\n",
    "            reviewCount = raiting.text.replace('\\n', '').strip().split(' ')[0].replace(',', '')\n",
    "    \n",
    "    #extract the book title\n",
    "    bookTitle = soup.find_all('h1')[0].contents[0].replace('\\n', '').strip()\n",
    "\n",
    "    #extract the book authors\n",
    "    bookAuthors = soup.find_all('span', itemprop='name')[0].contents[0]\n",
    "    \n",
    "    #extract the book authors, we shoul FIX it.\n",
    "    try:\n",
    "        Plot = soup.find_all('div', id=\"description\")[0].contents[3].text\n",
    "    except:\n",
    "        Plot = soup.find_all('div', id=\"description\")\n",
    "        if not Plot:\n",
    "            Plot = \"NaN\"\n",
    "        else:\n",
    "            Plot = soup.find_all('div', id=\"description\")[0].contents[1].text\n",
    "    if detect(Plot) != \"en\":\n",
    "        Plot = \"NaN\"\n",
    "    \n",
    "    #extract the date\n",
    "    date = soup.find_all('div', id=\"details\")[0].contents[3].text.replace('\\n', '').strip().split()\n",
    "    Published = date[1]+\" \"+date[2]+\" \"+date[3]\n",
    "    \n",
    "    #Rating Value\n",
    "    ratingValue = soup.find('span', itemprop=\"ratingValue\").text.strip()\n",
    "    \n",
    "    #Number of pages\n",
    "    NumberofPages = soup.find('span', itemprop=\"numberOfPages\").text.split()[0]\n",
    "    \n",
    "    #Title series\n",
    "    bookSeries = soup.find_all('a', href= re.compile(r'/series/*'))\n",
    "    if not bookSeries:\n",
    "        bookSeries = \"NaN\"\n",
    "    else:\n",
    "        bookSeries = soup.find_all('a', href= re.compile(r'/series/*'))[0].contents[0].strip()\n",
    "    \n",
    "    #Places\n",
    "    Setting = []\n",
    "    for places in soup.find_all('a', href= re.compile(r'/places/*')):\n",
    "        Setting.append(places.text)\n",
    "    Setting = \", \".join(Setting) if len(Setting)>=1 else \"NaN\"\n",
    "        \n",
    "    #list of characters\n",
    "    Characters = []\n",
    "    for character in soup.find_all('a', href= re.compile(r'/characters/*') ):\n",
    "        Characters.append(character.text)\n",
    "    Characters = \", \".join(Characters) if len(Characters)>=1 else \"NaN\"\n",
    "    \n",
    "    tsv_writer.writerow([bookTitle, bookSeries, bookAuthors, ratingValue, ratingCount, reviewCount, Plot, NumberofPages, Published, Characters, Setting, Url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "UrlsFiles = open(\"urlpages.txt\", \"r\")\n",
    "\n",
    "with open('./output.tsv', 'w', encoding=\"utf-8\", newline='') as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "    tsv_writer.writerow(['bookTitle', 'bookSeries', 'bookAuthors', 'ratingValue', \n",
    "                    'ratingCount', 'reviewCount', 'Plot', 'NumberofPages', 'Published',\n",
    "                    'Characters', 'Setting', 'Url'])\n",
    "    for link in UrlsFiles:\n",
    "        scrap_book(\"https://www.goodreads.com\" + link, tsv_writer, link)\n",
    "\n",
    "UrlsFiles.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ[2] Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('output.tsv',sep=\"\\t\")\n",
    "df2 = pd.read_csv('output2.tsv',sep=\"\\t\")\n",
    "df3 = pd.read_csv('output3.tsv',sep=\"\\t\")\n",
    "\n",
    "complete_dataset = pd.concat([df1, df2, df3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>bookSeries</th>\n",
       "      <th>bookAuthors</th>\n",
       "      <th>ratingValue</th>\n",
       "      <th>ratingCount</th>\n",
       "      <th>reviewCount</th>\n",
       "      <th>Plot</th>\n",
       "      <th>NumberofPages</th>\n",
       "      <th>Published</th>\n",
       "      <th>Characters</th>\n",
       "      <th>Setting</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>(The Hunger Games #1)</td>\n",
       "      <td>Suzanne Collins</td>\n",
       "      <td>4.33</td>\n",
       "      <td>6409993</td>\n",
       "      <td>172577</td>\n",
       "      <td>Could you survive on your own in the wild, wit...</td>\n",
       "      <td>374</td>\n",
       "      <td>September 14th 2008</td>\n",
       "      <td>Katniss Everdeen, Peeta Mellark, Cato (Hunger ...</td>\n",
       "      <td>District 12, Panem, Capitol, Panem, Panem</td>\n",
       "      <td>/book/show/2767052-the-hunger-games\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>(Harry Potter #5)</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>4.50</td>\n",
       "      <td>2525510</td>\n",
       "      <td>42743</td>\n",
       "      <td>There is a door at the end of a silent corrido...</td>\n",
       "      <td>870</td>\n",
       "      <td>September 2004 by</td>\n",
       "      <td>Sirius Black, Draco Malfoy, Ron Weasley, Petun...</td>\n",
       "      <td>Hogwarts School of Witchcraft and Wizardry, Lo...</td>\n",
       "      <td>/book/show/2.Harry_Potter_and_the_Order_of_the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>(To Kill a Mockingbird)</td>\n",
       "      <td>Harper Lee</td>\n",
       "      <td>4.28</td>\n",
       "      <td>4528131</td>\n",
       "      <td>91816</td>\n",
       "      <td>The unforgettable novel of a childhood in a sl...</td>\n",
       "      <td>324</td>\n",
       "      <td>May 23rd 2006</td>\n",
       "      <td>Scout Finch, Atticus Finch, Jem Finch, Arthur ...</td>\n",
       "      <td>Maycomb, Alabama</td>\n",
       "      <td>/book/show/2657.To_Kill_a_Mockingbird\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>4.26</td>\n",
       "      <td>3018223</td>\n",
       "      <td>67820</td>\n",
       "      <td>Alternate cover edition of ISBN 9780679783268S...</td>\n",
       "      <td>279</td>\n",
       "      <td>October 10th 2000</td>\n",
       "      <td>Mr. Bennet, Mrs. Bennet, Jane Bennet, Elizabet...</td>\n",
       "      <td>United Kingdom, Derbyshire, England, England, ...</td>\n",
       "      <td>/book/show/1885.Pride_and_Prejudice\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twilight</td>\n",
       "      <td>(The Twilight Saga #1)</td>\n",
       "      <td>Stephenie Meyer</td>\n",
       "      <td>3.60</td>\n",
       "      <td>4990866</td>\n",
       "      <td>104922</td>\n",
       "      <td>About three things I was absolutely positive.F...</td>\n",
       "      <td>501</td>\n",
       "      <td>September 6th 2006</td>\n",
       "      <td>Edward Cullen, Jacob Black, Laurent, Renee, Be...</td>\n",
       "      <td>Forks, Washington, Phoenix, Arizona, Washingto...</td>\n",
       "      <td>/book/show/41865.Twilight\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>The Things They Carried</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tim O'Brien</td>\n",
       "      <td>4.13</td>\n",
       "      <td>255296</td>\n",
       "      <td>14507</td>\n",
       "      <td>In 1979, Tim O'Brien's Going After Cacciato—a ...</td>\n",
       "      <td>246</td>\n",
       "      <td>December 29th 1998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/book/show/133518.The_Things_They_Carried\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Foundation</td>\n",
       "      <td>(Foundation (Publication Order) #1)</td>\n",
       "      <td>Isaac Asimov</td>\n",
       "      <td>4.17</td>\n",
       "      <td>413269</td>\n",
       "      <td>9857</td>\n",
       "      <td>For twelve thousand years the Galactic Empire ...</td>\n",
       "      <td>244</td>\n",
       "      <td>June 1st 2004</td>\n",
       "      <td>Hari Seldon, Hober Mallow, Salvor Hardin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/book/show/29579.Foundation\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>The Complete Works</td>\n",
       "      <td>NaN</td>\n",
       "      <td>William Shakespeare</td>\n",
       "      <td>4.50</td>\n",
       "      <td>50226</td>\n",
       "      <td>846</td>\n",
       "      <td>Tempest\\tTwo Gentlemen of Verona\\tMerry Wives ...</td>\n",
       "      <td>1248</td>\n",
       "      <td>September 8th 1990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/book/show/569564.The_Complete_Works\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>The Thirteenth Tale</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Diane Setterfield</td>\n",
       "      <td>3.96</td>\n",
       "      <td>273732</td>\n",
       "      <td>23026</td>\n",
       "      <td>All children mythologize their birth...So begi...</td>\n",
       "      <td>406</td>\n",
       "      <td>September 12th 2006</td>\n",
       "      <td>Margaret Lea, Emmeline March, Adeline March, V...</td>\n",
       "      <td>Angelfield</td>\n",
       "      <td>/book/show/40440.The_Thirteenth_Tale\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Norwegian Wood</td>\n",
       "      <td>(ノルウェイの森)</td>\n",
       "      <td>Haruki Murakami</td>\n",
       "      <td>4.03</td>\n",
       "      <td>350983</td>\n",
       "      <td>21150</td>\n",
       "      <td>Toru, a quiet and preternaturally serious youn...</td>\n",
       "      <td>296</td>\n",
       "      <td>September 12th 2000</td>\n",
       "      <td>Toru Watanabe, Naoko, Midori Kobayashi, Reiko ...</td>\n",
       "      <td>Tokyo, Japan</td>\n",
       "      <td>/book/show/11297.Norwegian_Wood\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    bookTitle  \\\n",
       "0                            The Hunger Games   \n",
       "1   Harry Potter and the Order of the Phoenix   \n",
       "2                       To Kill a Mockingbird   \n",
       "3                         Pride and Prejudice   \n",
       "4                                    Twilight   \n",
       "..                                        ...   \n",
       "95                    The Things They Carried   \n",
       "96                                 Foundation   \n",
       "97                         The Complete Works   \n",
       "98                        The Thirteenth Tale   \n",
       "99                             Norwegian Wood   \n",
       "\n",
       "                             bookSeries          bookAuthors  ratingValue  \\\n",
       "0                 (The Hunger Games #1)      Suzanne Collins         4.33   \n",
       "1                     (Harry Potter #5)         J.K. Rowling         4.50   \n",
       "2               (To Kill a Mockingbird)           Harper Lee         4.28   \n",
       "3                                   NaN          Jane Austen         4.26   \n",
       "4                (The Twilight Saga #1)      Stephenie Meyer         3.60   \n",
       "..                                  ...                  ...          ...   \n",
       "95                                  NaN          Tim O'Brien         4.13   \n",
       "96  (Foundation (Publication Order) #1)         Isaac Asimov         4.17   \n",
       "97                                  NaN  William Shakespeare         4.50   \n",
       "98                                  NaN    Diane Setterfield         3.96   \n",
       "99                            (ノルウェイの森)      Haruki Murakami         4.03   \n",
       "\n",
       "    ratingCount  reviewCount  \\\n",
       "0       6409993       172577   \n",
       "1       2525510        42743   \n",
       "2       4528131        91816   \n",
       "3       3018223        67820   \n",
       "4       4990866       104922   \n",
       "..          ...          ...   \n",
       "95       255296        14507   \n",
       "96       413269         9857   \n",
       "97        50226          846   \n",
       "98       273732        23026   \n",
       "99       350983        21150   \n",
       "\n",
       "                                                 Plot  NumberofPages  \\\n",
       "0   Could you survive on your own in the wild, wit...            374   \n",
       "1   There is a door at the end of a silent corrido...            870   \n",
       "2   The unforgettable novel of a childhood in a sl...            324   \n",
       "3   Alternate cover edition of ISBN 9780679783268S...            279   \n",
       "4   About three things I was absolutely positive.F...            501   \n",
       "..                                                ...            ...   \n",
       "95  In 1979, Tim O'Brien's Going After Cacciato—a ...            246   \n",
       "96  For twelve thousand years the Galactic Empire ...            244   \n",
       "97  Tempest\\tTwo Gentlemen of Verona\\tMerry Wives ...           1248   \n",
       "98  All children mythologize their birth...So begi...            406   \n",
       "99  Toru, a quiet and preternaturally serious youn...            296   \n",
       "\n",
       "              Published                                         Characters  \\\n",
       "0   September 14th 2008  Katniss Everdeen, Peeta Mellark, Cato (Hunger ...   \n",
       "1     September 2004 by  Sirius Black, Draco Malfoy, Ron Weasley, Petun...   \n",
       "2         May 23rd 2006  Scout Finch, Atticus Finch, Jem Finch, Arthur ...   \n",
       "3     October 10th 2000  Mr. Bennet, Mrs. Bennet, Jane Bennet, Elizabet...   \n",
       "4    September 6th 2006  Edward Cullen, Jacob Black, Laurent, Renee, Be...   \n",
       "..                  ...                                                ...   \n",
       "95   December 29th 1998                                                NaN   \n",
       "96        June 1st 2004           Hari Seldon, Hober Mallow, Salvor Hardin   \n",
       "97   September 8th 1990                                                NaN   \n",
       "98  September 12th 2006  Margaret Lea, Emmeline March, Adeline March, V...   \n",
       "99  September 12th 2000  Toru Watanabe, Naoko, Midori Kobayashi, Reiko ...   \n",
       "\n",
       "                                              Setting  \\\n",
       "0           District 12, Panem, Capitol, Panem, Panem   \n",
       "1   Hogwarts School of Witchcraft and Wizardry, Lo...   \n",
       "2                                    Maycomb, Alabama   \n",
       "3   United Kingdom, Derbyshire, England, England, ...   \n",
       "4   Forks, Washington, Phoenix, Arizona, Washingto...   \n",
       "..                                                ...   \n",
       "95                                                NaN   \n",
       "96                                                NaN   \n",
       "97                                                NaN   \n",
       "98                                         Angelfield   \n",
       "99                                       Tokyo, Japan   \n",
       "\n",
       "                                                  Url  \n",
       "0               /book/show/2767052-the-hunger-games\\n  \n",
       "1   /book/show/2.Harry_Potter_and_the_Order_of_the...  \n",
       "2             /book/show/2657.To_Kill_a_Mockingbird\\n  \n",
       "3               /book/show/1885.Pride_and_Prejudice\\n  \n",
       "4                         /book/show/41865.Twilight\\n  \n",
       "..                                                ...  \n",
       "95        /book/show/133518.The_Things_They_Carried\\n  \n",
       "96                      /book/show/29579.Foundation\\n  \n",
       "97             /book/show/569564.The_Complete_Works\\n  \n",
       "98             /book/show/40440.The_Thirteenth_Tale\\n  \n",
       "99                  /book/show/11297.Norwegian_Wood\\n  \n",
       "\n",
       "[300 rows x 12 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove NaN plot\n",
    "complete_dataset = complete_dataset[complete_dataset[\"Plot\"].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Cleaning data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DEF Cleaning\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop = set(stopwords.words('english')) # words in english to avoid few data for the cleaning data\n",
    "\n",
    "# We use lemmatizer and stemmer to obtain the root of the word\n",
    "lemmatizer = WordNetLemmatizer() # non contesto\n",
    "stemmer = PorterStemmer() # contesto\n",
    "\n",
    "vocabulary = {}\n",
    "df = complete_dataset.copy()\n",
    "for i, row in enumerate(df[\"Plot\"], 1):\n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\")  # import the tokenizer punctuation\n",
    "    row = tokenizer.tokenize(row.lower()) # remove the punctuation\n",
    "    \n",
    "    row = list(set(row) - stop) # remove the stop words\n",
    "    \n",
    "    row = [lemmatizer.lemmatize(word) for word in row]\n",
    "    row = [stemmer.stem(word) for word in row]\n",
    "    \n",
    "    for word in row:\n",
    "        if word in vocabulary.keys():\n",
    "            if \"document_\"+str(i) not in vocabulary[word]:\n",
    "                vocabulary[word].append(\"document_\"+str(i))\n",
    "        else:\n",
    "            vocabulary[word] = [\"document_\"+str(i)]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create file .csv\n",
    "import csv\n",
    "\n",
    "with open('./inverted_list.tsv', 'w', encoding=\"utf-8\", newline='') as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "    tsv_writer.writerow([\"Word\", \"Term_id\", \"Document_List\"])\n",
    "    \n",
    "    list_of_words = vocabulary.keys()\n",
    "    for id, word in enumerate(list_of_words, 1):\n",
    "        term_id_i = \"term_id_\"+str(id)\n",
    "        tsv_writer.writerow([word, term_id_i, vocabulary[word]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ[2.1.2] Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query string: Survival YounGer\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Insert your query string: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "viva la patacca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanQuery(row):\n",
    "    row = row.split(\" \") # split the string query\n",
    "    for element in row:\n",
    "        tokenizer = RegexpTokenizer(\"[\\w']+\")  # import the tokenizer punctuation\n",
    "        element = tokenizer.tokenize(element.lower()) # remove the punctuation\n",
    "    row = list(set(row) - stop) # remove the stop words\n",
    "    \n",
    "    row = [lemmatizer.lemmatize(word) for word in row]\n",
    "    row = [stemmer.stem(word) for word in row]\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['surviv', 'younger']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanQString = cleanQuery(query)\n",
    "cleanQString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "open the dataset vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_lst = pd.read_csv('inverted_list.tsv',sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Term_id</th>\n",
       "      <th>Document_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>north</td>\n",
       "      <td>term_id_1</td>\n",
       "      <td>['document_1', 'document_46', 'document_90', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lie</td>\n",
       "      <td>term_id_2</td>\n",
       "      <td>['document_1', 'document_42', 'document_44', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>contend</td>\n",
       "      <td>term_id_3</td>\n",
       "      <td>['document_1', 'document_196']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>katniss</td>\n",
       "      <td>term_id_4</td>\n",
       "      <td>['document_1', 'document_223']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>live</td>\n",
       "      <td>term_id_5</td>\n",
       "      <td>['document_1', 'document_21', 'document_22', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word    Term_id                                      Document_List\n",
       "0    north  term_id_1  ['document_1', 'document_46', 'document_90', '...\n",
       "1      lie  term_id_2  ['document_1', 'document_42', 'document_44', '...\n",
       "2  contend  term_id_3                     ['document_1', 'document_196']\n",
       "3  katniss  term_id_4                     ['document_1', 'document_223']\n",
       "4     live  term_id_5  ['document_1', 'document_21', 'document_22', '..."
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_lst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_lst.set_index('Word', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term_id</th>\n",
       "      <th>Document_List</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>north</th>\n",
       "      <td>term_id_1</td>\n",
       "      <td>['document_1', 'document_46', 'document_90', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lie</th>\n",
       "      <td>term_id_2</td>\n",
       "      <td>['document_1', 'document_42', 'document_44', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contend</th>\n",
       "      <td>term_id_3</td>\n",
       "      <td>['document_1', 'document_196']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>katniss</th>\n",
       "      <td>term_id_4</td>\n",
       "      <td>['document_1', 'document_223']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>live</th>\n",
       "      <td>term_id_5</td>\n",
       "      <td>['document_1', 'document_21', 'document_22', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Term_id                                      Document_List\n",
       "Word                                                                 \n",
       "north    term_id_1  ['document_1', 'document_46', 'document_90', '...\n",
       "lie      term_id_2  ['document_1', 'document_42', 'document_44', '...\n",
       "contend  term_id_3                     ['document_1', 'document_196']\n",
       "katniss  term_id_4                     ['document_1', 'document_223']\n",
       "live     term_id_5  ['document_1', 'document_21', 'document_22', '..."
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_lst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_lists = inv_lst.loc[cleanQString, \"Document_List\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"['document_1', 'document_32', 'document_45', 'document_53', 'document_54', 'document_56', 'document_68', 'document_70', 'document_77', 'document_86', 'document_151', 'document_168', 'document_177', 'document_181', 'document_188', 'document_209', 'document_214', 'document_223', 'document_230', 'document_238', 'document_263']\",\n",
       "       \"['document_1', 'document_182']\"], dtype=object)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_lists.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

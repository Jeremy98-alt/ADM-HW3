{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import lxml #is a parser for HTMl\n",
    "import webscraping\n",
    "import requests\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shoul generalize these functions... in webscraping.py!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **[RQ1.1] Get the list of books**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start to scrape the three pages to extract the URLs for each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "UrlsFiles = open(\"urlpages.txt\", \"w\")\n",
    "\n",
    "url = \"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"+str(1)\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, features='lxml')\n",
    "for a in soup.find_all('a', class_=\"bookTitle\"): #.contents[0].split('\\n')[1].strip()\n",
    "    UrlsFiles.write(a.get('href')+'\\n')\n",
    "\n",
    "#UrlsFiles.truncate(UrlsFiles.tell()-1) see how to remove the '\\n'\n",
    "UrlsFiles.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RQ[1.2] Crawl books**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "headpart = \"https://www.goodreads.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = \"htmlpages\" #Directory  \n",
    "parent_dir = \"./\" #Parent Directory path \n",
    "pathAncestor = os.path.join(parent_dir, direct) #Path\n",
    "os.mkdir(pathAncestor) #create the folder in the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,101):\n",
    "    os.makedirs(os.path.join(pathAncestor, 'page ' + str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "UrlsFiles = open(\"urlpages.txt\", \"r\")\n",
    "\n",
    "for i, x in enumerate(UrlsFiles,1):\n",
    "    subdirectory = pathAncestor + \"/page \" + str(i)\n",
    "    article_name = \"/article_\"+str(i)+\".html\"\n",
    "    \n",
    "    complete_path = subdirectory + article_name\n",
    "    with open(complete_path, \"wb\") as ip_file:\n",
    "        link = headpart + x\n",
    "        page = requests.get(link)\n",
    "        \n",
    "        soup = BeautifulSoup(page.text, features='lxml')\n",
    "        \n",
    "        ip_file.write(soup.encode('utf-8'))\n",
    "        ip_file.close()\n",
    "\n",
    "UrlsFiles.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RQ[1.3] Parse downloaded pages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_book(href):\n",
    "    r = requests.get(href)\n",
    "    soup = BeautifulSoup(\n",
    "        r.text,\n",
    "        features=\"lxml\"\n",
    "    )\n",
    "    \n",
    "    book_title = soup.find_all('h1')[0].contents[0].replace('\\n', '').strip()\n",
    "    \n",
    "    author = soup.find_all('span', itemprop='name')[0].contents[0]\n",
    "    \n",
    "    print(\"Description:\\n\") #show not all but with more link..\n",
    "    description = ''.join([c for c in soup.find_all('div', id=\"description\")[0].contents[1].text if c != '</br>'])\n",
    "    print(description)\n",
    "    \n",
    "    print(\"\\nDate:\")\n",
    "    date = soup.find_all('div', id=\"details\")[0].contents[3].text.replace('\\n', '').strip().split()\n",
    "    print(date[1]+\" \"+date[2]+\" \"+date[3])\n",
    "    \n",
    "    ratings = soup.find_all('a', href=\"#other_reviews\")\n",
    "    rating_count = -1\n",
    "    rating = -1\n",
    "    for raiting in ratings:\n",
    "        if raiting.find_all('meta', itemprop=\"ratingCount\"):\n",
    "            rating_count = raiting.text.replace('\\n', '').strip().split(' ')[0].replace(',', '')\n",
    "        elif raiting.find_all('meta', itemprop=\"reviewCount\"):\n",
    "            rating = raiting.text.replace('\\n', '').strip().split(' ')[0].replace(',', '')\n",
    "    \n",
    "    print(f\"\\nbook_title: {book_title}\\nauthor: {author}\\nratings: {rating_count}\\nreviews: {rating}\")\n",
    "    \n",
    "    #Rating Value\n",
    "    ratingValue = soup.find('span', itemprop=\"ratingValue\")\n",
    "    print(\"rating value:\", ratingValue.text.strip())\n",
    "    \n",
    "    #Number of pages\n",
    "    numberOfPages = soup.find('span', itemprop=\"numberOfPages\")\n",
    "    print(\"numberOfPages:\", numberOfPages.text.strip())\n",
    "    \n",
    "    #Title series\n",
    "    series = soup.find_all('a', href= re.compile(r'/series/*'))[0].contents[0].strip()\n",
    "    print(\"Title Series:\",series)\n",
    "    \n",
    "    #Places\n",
    "    print(\"\\nSetting:\")\n",
    "    for places in soup.find_all('a', href= re.compile(r'/places/*')):\n",
    "        print(places.text)\n",
    "        \n",
    "    #list of characters\n",
    "    print(\"\\nCharacters:\")\n",
    "    for characters in soup.find_all('a', href= re.compile(r'/characters/*') ):\n",
    "        print(characters.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description:\n",
      "\n",
      "Could you survive on your own in the wild, with every one out to make sure you don't live to see the morning?In the ruins of a place once known as North America lies the nation of Panem, a shining Capitol surrounded by twelve outlying districts. The Capitol is harsh and cruel and keeps the districts in line by forcing them all to send one boy and one girl between the ages \n",
      "\n",
      "Date:\n",
      "September 14th 2008\n",
      "\n",
      "book_title: The Hunger Games\n",
      "author: Suzanne Collins\n",
      "ratings: 6408094\n",
      "reviews: 172545\n",
      "rating value: 4.33\n",
      "numberOfPages: 374 pages\n",
      "Title Series: (The Hunger Games #1)\n",
      "\n",
      "Setting:\n",
      "District 12, Panem\n",
      "Capitol, Panem\n",
      "Panem\n",
      "\n",
      "Characters:\n",
      "Katniss Everdeen\n",
      "Peeta Mellark\n",
      "Cato (Hunger Games)\n",
      "Primrose Everdeen\n",
      "Gale Hawthorne\n",
      "Effie Trinket\n",
      "Haymitch Abernathy\n",
      "Cinna\n",
      "President Coriolanus Snow\n",
      "Rue\n",
      "Flavius\n",
      "Lavinia (Hunger Games)\n",
      "Marvel\n",
      "Glimmer\n",
      "Clove\n",
      "Foxface\n",
      "Thresh\n",
      "Greasy Sae\n",
      "Madge Undersee\n",
      "Caesar Flickerman\n",
      "Claudius Templesmith\n",
      "Octavia (Hunger Games)\n",
      "Portia (hunger Games)\n"
     ]
    }
   ],
   "source": [
    "scrap_book(\"https://www.goodreads.com/book/show/2767052-the-hunger-games\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* check input and returns in functions\n",
    "* check that what i wrote is accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leonardo Masci, Jeremy Sapienza, Antonio Zappia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this homework is to create a search engine that returns some book suggestions, given an input from the user in the form of words. To achieve this, we first scraped the website Goodreads.com, in its \"best books ever\" section, to get a dataset of 30,000 books. Once we had our dataset, we created increasingly better search algorithms to get to a satisfying result. \n",
    "\n",
    "Finally, in the last exercise we faced a recursive algorithm and its running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since no dataset was provided for this homework, we had to create our own dataset by scraping the given website, which was done in the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get the list of books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we had to get the list of all the books we wanted to add to our dataset. This included all the books found in the first 300 pages of the \"best books ever\" section.\n",
    "Therefore, we created a function that scrapes a certain number of pages from an initial url, to extract the url of each of them. The initial url and the following number of pages are given as input, while the resulting urls are stored in a txt file called \"urlpages.txt\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.get_urls(\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\", 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Crawl books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we had our txt file containing all the urls, we created a second function that downloads the html of each given url. These htmls are collected in different folders, depending on the corresponding page. Moreover, we added a try and except to be sure that all requests that we could lose will be re-scrape in a second moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.crawl_urls(300,\"urlpages.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after completing this step, we went on to actually parse the given html articles. We will obtain for each html article its article_i.tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.get_scraping(\"html_pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and concatening the final dataset for our purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since running the above mentioned functions took a long time, the three of us split the work among us. So in the end we run another function that returned our distinct outputs as tsv files and combined those three to create the final dataset we will be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.finaldataset_tsv(html_pages,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset, we can start working on the search engines we are interested in. We will start with a very simple one and work our way towards a more complex one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "df = functions.openMainDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We open the final dataset to do our operations. Now, we illustrate the main features and we show a piece of dataset belove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>bookSeries</th>\n",
       "      <th>bookAuthors</th>\n",
       "      <th>ratingValue</th>\n",
       "      <th>ratingCount</th>\n",
       "      <th>reviewCount</th>\n",
       "      <th>Plot</th>\n",
       "      <th>NumberofPages</th>\n",
       "      <th>Published</th>\n",
       "      <th>Characters</th>\n",
       "      <th>Setting</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>(The Hunger Games #1)</td>\n",
       "      <td>Suzanne Collins</td>\n",
       "      <td>4.33</td>\n",
       "      <td>6413302</td>\n",
       "      <td>172615</td>\n",
       "      <td>Could you survive on your own in the wild, wit...</td>\n",
       "      <td>374</td>\n",
       "      <td>September 14th 2008</td>\n",
       "      <td>Katniss Everdeen, Peeta Mellark, Cato (Hunger ...</td>\n",
       "      <td>District 12, Panem, Capitol, Panem, Panem</td>\n",
       "      <td>https://www.goodreads.com/book/show/2767052-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>(Harry Potter #5)</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>4.50</td>\n",
       "      <td>2527001</td>\n",
       "      <td>42768</td>\n",
       "      <td>There is a door at the end of a silent corrido...</td>\n",
       "      <td>870</td>\n",
       "      <td>September 2004 by</td>\n",
       "      <td>Sirius Black, Draco Malfoy, Ron Weasley, Petun...</td>\n",
       "      <td>Hogwarts School of Witchcraft and Wizardry, Lo...</td>\n",
       "      <td>https://www.goodreads.com/book/show/2.Harry_Po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>(To Kill a Mockingbird)</td>\n",
       "      <td>Harper Lee</td>\n",
       "      <td>4.28</td>\n",
       "      <td>4530963</td>\n",
       "      <td>91866</td>\n",
       "      <td>The unforgettable novel of a childhood in a sl...</td>\n",
       "      <td>324</td>\n",
       "      <td>May 23rd 2006</td>\n",
       "      <td>Scout Finch, Atticus Finch, Jem Finch, Arthur ...</td>\n",
       "      <td>Maycomb, Alabama</td>\n",
       "      <td>https://www.goodreads.com/book/show/2657.To_Ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td></td>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>4.26</td>\n",
       "      <td>3020392</td>\n",
       "      <td>67869</td>\n",
       "      <td>Alternate cover edition of ISBN 9780679783268S...</td>\n",
       "      <td>279</td>\n",
       "      <td>October 10th 2000</td>\n",
       "      <td>Mr. Bennet, Mrs. Bennet, Jane Bennet, Elizabet...</td>\n",
       "      <td>United Kingdom, Derbyshire, England, England, ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/1885.Pride...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twilight</td>\n",
       "      <td>(The Twilight Saga #1)</td>\n",
       "      <td>Stephenie Meyer</td>\n",
       "      <td>3.60</td>\n",
       "      <td>4993492</td>\n",
       "      <td>104954</td>\n",
       "      <td>About three things I was absolutely positive.F...</td>\n",
       "      <td>501</td>\n",
       "      <td>September 6th 2006</td>\n",
       "      <td>Edward Cullen, Jacob Black, Laurent, Renee, Be...</td>\n",
       "      <td>Forks, Washington, Phoenix, Arizona, Washingto...</td>\n",
       "      <td>https://www.goodreads.com/book/show/41865.Twil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   bookTitle               bookSeries  \\\n",
       "0                           The Hunger Games    (The Hunger Games #1)   \n",
       "1  Harry Potter and the Order of the Phoenix        (Harry Potter #5)   \n",
       "2                      To Kill a Mockingbird  (To Kill a Mockingbird)   \n",
       "3                        Pride and Prejudice                            \n",
       "4                                   Twilight   (The Twilight Saga #1)   \n",
       "\n",
       "       bookAuthors  ratingValue  ratingCount  reviewCount  \\\n",
       "0  Suzanne Collins         4.33      6413302       172615   \n",
       "1     J.K. Rowling         4.50      2527001        42768   \n",
       "2       Harper Lee         4.28      4530963        91866   \n",
       "3      Jane Austen         4.26      3020392        67869   \n",
       "4  Stephenie Meyer         3.60      4993492       104954   \n",
       "\n",
       "                                                Plot NumberofPages  \\\n",
       "0  Could you survive on your own in the wild, wit...           374   \n",
       "1  There is a door at the end of a silent corrido...           870   \n",
       "2  The unforgettable novel of a childhood in a sl...           324   \n",
       "3  Alternate cover edition of ISBN 9780679783268S...           279   \n",
       "4  About three things I was absolutely positive.F...           501   \n",
       "\n",
       "             Published                                         Characters  \\\n",
       "0  September 14th 2008  Katniss Everdeen, Peeta Mellark, Cato (Hunger ...   \n",
       "1    September 2004 by  Sirius Black, Draco Malfoy, Ron Weasley, Petun...   \n",
       "2        May 23rd 2006  Scout Finch, Atticus Finch, Jem Finch, Arthur ...   \n",
       "3    October 10th 2000  Mr. Bennet, Mrs. Bennet, Jane Bennet, Elizabet...   \n",
       "4   September 6th 2006  Edward Cullen, Jacob Black, Laurent, Renee, Be...   \n",
       "\n",
       "                                             Setting  \\\n",
       "0          District 12, Panem, Capitol, Panem, Panem   \n",
       "1  Hogwarts School of Witchcraft and Wizardry, Lo...   \n",
       "2                                   Maycomb, Alabama   \n",
       "3  United Kingdom, Derbyshire, England, England, ...   \n",
       "4  Forks, Washington, Phoenix, Arizona, Washingto...   \n",
       "\n",
       "                                                 Url  \n",
       "0  https://www.goodreads.com/book/show/2767052-th...  \n",
       "1  https://www.goodreads.com/book/show/2.Harry_Po...  \n",
       "2  https://www.goodreads.com/book/show/2657.To_Ki...  \n",
       "3  https://www.goodreads.com/book/show/1885.Pride...  \n",
       "4  https://www.goodreads.com/book/show/41865.Twil...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we collect the main features according to the professor requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratingValue</th>\n",
       "      <th>ratingCount</th>\n",
       "      <th>reviewCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25442.000000</td>\n",
       "      <td>2.544200e+04</td>\n",
       "      <td>25442.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.045976</td>\n",
       "      <td>3.299154e+04</td>\n",
       "      <td>1753.184223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.358727</td>\n",
       "      <td>1.486322e+05</td>\n",
       "      <td>5401.072115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.850000</td>\n",
       "      <td>7.642500e+02</td>\n",
       "      <td>69.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.050000</td>\n",
       "      <td>5.747000e+03</td>\n",
       "      <td>374.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.240000</td>\n",
       "      <td>2.098875e+04</td>\n",
       "      <td>1333.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.104838e+06</td>\n",
       "      <td>172615.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ratingValue   ratingCount    reviewCount\n",
       "count  25442.000000  2.544200e+04   25442.000000\n",
       "mean       4.045976  3.299154e+04    1753.184223\n",
       "std        0.358727  1.486322e+05    5401.072115\n",
       "min        0.000000  0.000000e+00       0.000000\n",
       "25%        3.850000  7.642500e+02      69.000000\n",
       "50%        4.050000  5.747000e+03     374.500000\n",
       "75%        4.240000  2.098875e+04    1333.000000\n",
       "max        5.000000  7.104838e+06  172615.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data and creating the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can procede with the search engines, we need to clean the dataset we have obtained and create a vocabulary. \n",
    "\n",
    "The clean data is needed to apply the search engine, as the input words may not match the ones in our file if processed raw. For this reason, we remove stopwords and punctuation, which do not constitute valuable information, and we apply stemming. This way, we are left with only the root of words, which will make matching in the coming functions that much easier.\n",
    "\n",
    "<font color=\"red\"> WRITE HERE THE MAIN STEPS OF BAG OF WORDS: remove punctution, what is stemming and when we apply this, why remove stop words and so on... </font>\n",
    "\n",
    "The vocabulary is needed for creating the inverted list as a dictionary and at the same time to map each word with its \"term_id_i\". We create this to avoid calculate every time this data structure.\n",
    "\n",
    "at this time cleaning the df_copy dataset in order to have the vocabulary based on \"Plot\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_copy = functions.cleaningDataset(df.copy()) # useful to obtain the vocabulary for each token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the dataset with apposite bag of words method, we create the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-6b1eca801f5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateVocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_copy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\ADM-HW3-leo\\functions.py\u001b[0m in \u001b[0;36mcreateVocabulary\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    278\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"document_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m                     \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"document_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m                 \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"document_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vocabulary = functions.createVocabulary(df_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid calculate again and again the vocabulary save it as .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.create_csv(vocabulary, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to receive the user's input, which is then processed the same way as the dataset was so that the matching will be possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query string: survival games\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Insert your query string: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the user's query to be sure match with our vocabulary tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanQString = functions.cleanQuery(query)\n",
    "cleanQString = [\"surviv\", \"game\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this separated from the rest of vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = functions.open_vocabulary(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the inverted list according to professor requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.inverted_list(vocabulary, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create the new search engine by using the clean query and the inverted list we have found in the previous steps. Now, we show the result according to the professor requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Term_id</th>\n",
       "      <th>Document_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>could</td>\n",
       "      <td>term_id_1</td>\n",
       "      <td>['document_0', 'document_7', 'document_31', 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>surviv</td>\n",
       "      <td>term_id_2</td>\n",
       "      <td>['document_0', 'document_31', 'document_44', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wild</td>\n",
       "      <td>term_id_3</td>\n",
       "      <td>['document_0', 'document_15', 'document_25', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>everi</td>\n",
       "      <td>term_id_4</td>\n",
       "      <td>['document_0', 'document_10', 'document_12', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one</td>\n",
       "      <td>term_id_5</td>\n",
       "      <td>['document_0', 'document_3', 'document_5', 'do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69158</th>\n",
       "      <td>kismet</td>\n",
       "      <td>term_id_69160</td>\n",
       "      <td>['document_25438']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69159</th>\n",
       "      <td>dunlap</td>\n",
       "      <td>term_id_69161</td>\n",
       "      <td>['document_25441']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69160</th>\n",
       "      <td>lolli</td>\n",
       "      <td>term_id_69162</td>\n",
       "      <td>['document_25441']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69161</th>\n",
       "      <td>coxfor</td>\n",
       "      <td>term_id_69163</td>\n",
       "      <td>['document_25441']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69162</th>\n",
       "      <td>69ing</td>\n",
       "      <td>term_id_69164</td>\n",
       "      <td>['document_25441']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69163 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word        Term_id  \\\n",
       "0       could      term_id_1   \n",
       "1      surviv      term_id_2   \n",
       "2        wild      term_id_3   \n",
       "3       everi      term_id_4   \n",
       "4         one      term_id_5   \n",
       "...       ...            ...   \n",
       "69158  kismet  term_id_69160   \n",
       "69159  dunlap  term_id_69161   \n",
       "69160   lolli  term_id_69162   \n",
       "69161  coxfor  term_id_69163   \n",
       "69162   69ing  term_id_69164   \n",
       "\n",
       "                                           Document_List  \n",
       "0      ['document_0', 'document_7', 'document_31', 'd...  \n",
       "1      ['document_0', 'document_31', 'document_44', '...  \n",
       "2      ['document_0', 'document_15', 'document_25', '...  \n",
       "3      ['document_0', 'document_10', 'document_12', '...  \n",
       "4      ['document_0', 'document_3', 'document_5', 'do...  \n",
       "...                                                  ...  \n",
       "69158                                 ['document_25438']  \n",
       "69159                                 ['document_25441']  \n",
       "69160                                 ['document_25441']  \n",
       "69161                                 ['document_25441']  \n",
       "69162                                 ['document_25441']  \n",
       "\n",
       "[69163 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Trap</td>\n",
       "      <td>From author Andrew Fukuda comes The Trap, the ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/17286729-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Naturals</td>\n",
       "      <td>Seventeen-year-old Cassie is a natural at read...</td>\n",
       "      <td>https://www.goodreads.com/book/show/13597723-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Gender Game</td>\n",
       "      <td>For fans of The Hunger Games and Divergent com...</td>\n",
       "      <td>https://www.goodreads.com/book/show/31131467-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Out Of Range</td>\n",
       "      <td>Game warden Joe Pickett returns in a twisting,...</td>\n",
       "      <td>https://www.goodreads.com/book/show/983546.Out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Southpaw</td>\n",
       "      <td>The Southpaw is a story about coming of age in...</td>\n",
       "      <td>https://www.goodreads.com/book/show/413736.The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The House of Shattered Wings</td>\n",
       "      <td>A superb murder mystery, on an epic scale, set...</td>\n",
       "      <td>https://www.goodreads.com/book/show/23601046-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Lucky One</td>\n",
       "      <td>When U.S. Marine Logan Thibault finds a photog...</td>\n",
       "      <td>https://www.goodreads.com/book/show/3063499-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jacinto's Remnant</td>\n",
       "      <td>Based on the blockbuster Xbox game, this is th...</td>\n",
       "      <td>https://www.goodreads.com/book/show/6276888-ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cybernetics Within Us</td>\n",
       "      <td>When planes mysteriously start crashing, the w...</td>\n",
       "      <td>https://www.goodreads.com/book/show/52494371-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Devil's Own</td>\n",
       "      <td>After surviving slavery, Aiden MacAlpin has no...</td>\n",
       "      <td>https://www.goodreads.com/book/show/8705483-de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      bookTitle  \\\n",
       "0                      The Trap   \n",
       "1                  The Naturals   \n",
       "2               The Gender Game   \n",
       "3                  Out Of Range   \n",
       "4                  The Southpaw   \n",
       "5  The House of Shattered Wings   \n",
       "6                 The Lucky One   \n",
       "7             Jacinto's Remnant   \n",
       "8         Cybernetics Within Us   \n",
       "9                   Devil's Own   \n",
       "\n",
       "                                                Plot  \\\n",
       "0  From author Andrew Fukuda comes The Trap, the ...   \n",
       "1  Seventeen-year-old Cassie is a natural at read...   \n",
       "2  For fans of The Hunger Games and Divergent com...   \n",
       "3  Game warden Joe Pickett returns in a twisting,...   \n",
       "4  The Southpaw is a story about coming of age in...   \n",
       "5  A superb murder mystery, on an epic scale, set...   \n",
       "6  When U.S. Marine Logan Thibault finds a photog...   \n",
       "7  Based on the blockbuster Xbox game, this is th...   \n",
       "8  When planes mysteriously start crashing, the w...   \n",
       "9  After surviving slavery, Aiden MacAlpin has no...   \n",
       "\n",
       "                                                 Url  \n",
       "0  https://www.goodreads.com/book/show/17286729-t...  \n",
       "1  https://www.goodreads.com/book/show/13597723-t...  \n",
       "2  https://www.goodreads.com/book/show/31131467-t...  \n",
       "3  https://www.goodreads.com/book/show/983546.Out...  \n",
       "4  https://www.goodreads.com/book/show/413736.The...  \n",
       "5  https://www.goodreads.com/book/show/23601046-t...  \n",
       "6  https://www.goodreads.com/book/show/3063499-th...  \n",
       "7  https://www.goodreads.com/book/show/6276888-ja...  \n",
       "8  https://www.goodreads.com/book/show/52494371-c...  \n",
       "9  https://www.goodreads.com/book/show/8705483-de...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions.search_engine1(cleanQString, 'inv_lst1.csv', vocabulary, df_copy, df).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before defining the new search engine, we have to update our inverted list according to professors request, we want to have the pair list, for each term that we have into our vocabulary, the document where the term is present and the relative tf_idf score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"> EXPLAIN WHERE DO YOU EXTRACT AND OBTAIN THE TF_IDF... EXPLAIN THE Term Frequency and the IDF... then the TF_IDF!! </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_lst2 = functions.new_inv_lst('inv_lst1.csv', vocabulary, df_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we get the user's input and we clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query string: survival games\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Insert your query string: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanQString = functions.cleanQuery(query)\n",
    "cleanQString = ['surviv', 'game']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHY NEW_DF? .... <font color=\"red\"> SHOW UP the new search engine according to the requests!... </font>\n",
    "\n",
    "We can now run the second search engine, which is an improved version of the previous one as it takes into consideration not only the number of words in the query that are found in the plots, but also how often those words are mentioned. This makes the search engine more accurate in identifying books that are relevant given a certain query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = functions.search_engine2(df_copy, df, inv_lst2, vocabulary, cleanQString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we show the final dataset returned by our search_engine2() function according to the professor requests.. we can see k top documents sorted by similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Halo: The Fall of Reach, The Flood, First Strike</td>\n",
       "      <td>\"The official novels of the award-winning Xbox...</td>\n",
       "      <td>https://www.goodreads.com/book/show/136179.Halo</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Devil's Own</td>\n",
       "      <td>After surviving slavery, Aiden MacAlpin has no...</td>\n",
       "      <td>https://www.goodreads.com/book/show/8705483-de...</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Most Dangerous Game</td>\n",
       "      <td>The Most Dangerous Game features a big-game hu...</td>\n",
       "      <td>https://www.goodreads.com/book/show/157076.The...</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Homestuck</td>\n",
       "      <td>It's a story about some kids who are friends o...</td>\n",
       "      <td>https://www.goodreads.com/book/show/20869872-h...</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Forbidden Game</td>\n",
       "      <td>When Jenny buys a game for her boyfriend, Tom,...</td>\n",
       "      <td>https://www.goodreads.com/book/show/7100490-th...</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          bookTitle  \\\n",
       "0  Halo: The Fall of Reach, The Flood, First Strike   \n",
       "1                                       Devil's Own   \n",
       "2                           The Most Dangerous Game   \n",
       "3                                         Homestuck   \n",
       "4                                The Forbidden Game   \n",
       "\n",
       "                                                Plot  \\\n",
       "0  \"The official novels of the award-winning Xbox...   \n",
       "1  After surviving slavery, Aiden MacAlpin has no...   \n",
       "2  The Most Dangerous Game features a big-game hu...   \n",
       "3  It's a story about some kids who are friends o...   \n",
       "4  When Jenny buys a game for her boyfriend, Tom,...   \n",
       "\n",
       "                                                 Url  Similarity  \n",
       "0    https://www.goodreads.com/book/show/136179.Halo        0.27  \n",
       "1  https://www.goodreads.com/book/show/8705483-de...        0.21  \n",
       "2  https://www.goodreads.com/book/show/157076.The...        0.21  \n",
       "3  https://www.goodreads.com/book/show/20869872-h...        0.18  \n",
       "4  https://www.goodreads.com/book/show/7100490-th...        0.18  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Define a new score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to improve again on our search engine, this time by taking into account multiple variables. \n",
    "* similarity score also in book title and author (year it was published, series, characters and setting)\n",
    "* we could define a \"popularity\" score that takes into account the average rating and the number of reviews\n",
    "* we then could combine the two to create the final score and sorting order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we get the user's input and we clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query string: SURVIVAL GAMES\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Insert your query string: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cleanQString = functions.cleanQuery(query)\n",
    "cleanQString = ['surviv', 'game']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new copy of the dataset according to our new score.\n",
    "\n",
    "The new score is based on four columns: Plot, bookTitle, bookSeries and bookAuthors. We want to use the normalize cosine similarity and sort the similar new score of cosine similarity according to ratingValue, ratingCount and reviewCount using the heap data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy2 = df_copy.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly remove the nan values in the other columns that we want to check and make the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_copy2['bookSeries'].replace(' ', np.nan, inplace=True)\n",
    "df_copy2['bookTitle'].replace(' ', np.nan, inplace=True)\n",
    "df_copy2['bookAuthors'].replace(' ', np.nan, inplace=True)\n",
    "\n",
    "#df_copy.dropna(subset=['bookSeries', 'bookTitle', 'bookAuthors'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the dataset according to the new added columns cited before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_copy2 = functions.newcleaningDataset(df_copy2) # useful to obtain the vocabulary for each token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the dataset with apposite bag of words method, we create the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "df_copy2 = pd.read_csv(\"token4Column.csv\", sep=\",\")\n",
    "df_copy = pd.read_csv(\"tokenOneColumn.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "vocabulary2 = functions.createVocabulary2(df_copy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid calculate again and again the vocabulary save it as .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.create_csv2(vocabulary2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the vocabulary and create the inverted list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary2 = functions.open_vocabulary(2)\n",
    "functions.inverted_list(vocabulary2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create the new search engine by using the clean query and the inverted list we have found in the previous steps. Now, we show the result according to the professor requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.search_engine3(cleanQString, 'inv_lst2.csv', vocabulary2, df_copy2, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the new score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We based on four columns to make the new score. So, we want to calculate the cosine similarity respect each column that we are considering previously. Then we normalize this four cosine similarity scores and we sorted the books according to the reviewCount, ratingValue and ratingCount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.search_engine4(cleanQString, 'inv_lst2.csv', vocabulary2, df_copy2, df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

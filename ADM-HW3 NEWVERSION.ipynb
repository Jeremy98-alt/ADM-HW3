{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import lxml #is a parser for HTMl\n",
    "import webscraping\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "#from selenium import webdriver\n",
    "from langdetect import detect\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shoul generalize these functions... in webscraping.py!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **[RQ1.1] Get the list of books**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start to scrape the three pages to extract the URLs for each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "UrlsFiles = open(\"urlpages.txt\", \"w\")\n",
    "\n",
    "for i in range(1, 101):\n",
    "    url = \"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"+str(i)\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, features='lxml')\n",
    "    for a in soup.find_all('a', class_=\"bookTitle\"):\n",
    "        UrlsFiles.write(a.get('href')+'\\n')\n",
    "\n",
    "UrlsFiles.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RQ[1.2] Crawl books**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "headpart = \"https://www.goodreads.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = \"htmlpages\" #Directory  \n",
    "parent_dir = \"./\" #Parent Directory path \n",
    "pathAncestor = os.path.join(parent_dir, direct) #Path\n",
    "os.mkdir(pathAncestor) #create the folder in the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,101):\n",
    "    os.makedirs(os.path.join(pathAncestor, 'page ' + str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "UrlsFiles = open(\"urlpages.txt\", \"r\")\n",
    "\n",
    "counter_pages = 0\n",
    "counter_html = 0\n",
    "for x in UrlsFiles:\n",
    "    if counter_html % 100 == 0:\n",
    "        counter_pages = counter_pages+1\n",
    "        \n",
    "    counter_html = counter_html + 1\n",
    "    \n",
    "    subdirectory = pathAncestor + \"/page \" + str(counter_pages)\n",
    "    article_name = \"/article_\"+str(counter_html)+\".html\"\n",
    "    \n",
    "    complete_path = subdirectory + article_name\n",
    "    with open(complete_path, \"wb\") as ip_file:\n",
    "        link = headpart + x\n",
    "        try:\n",
    "            page = requests.get(link)\n",
    "        except:\n",
    "            with open(\"failureRequest.txt\", \"wb\") as err_file:\n",
    "                err_file.write(link)\n",
    "                err_file.close()\n",
    "        \n",
    "        soup = BeautifulSoup(page.text, features='lxml')\n",
    "        \n",
    "        ip_file.write(soup.encode('utf-8'))\n",
    "        ip_file.close()\n",
    "\n",
    "UrlsFiles.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RQ[1.3] Parse downloaded pages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_book(tsv_writer, article):\n",
    "    global bookTitle, bookSeries, bookAuthors, ratingValue, ratingCount, reviewCount, Plot, NumberofPages, Published, Characters, Setting, Url;\n",
    "    \n",
    "    with open(article, 'r', encoding=\"utf-8\") as out_file:\n",
    "        contents = out_file.read()\n",
    "        soup = BeautifulSoup(contents, features=\"lxml\") #parse the text\n",
    "\n",
    "        #extract rating and review count\n",
    "        try:\n",
    "            ratings = soup.find_all('a', href=\"#other_reviews\") #search the ratings in its\n",
    "            rating_count = -1\n",
    "            rating = -1\n",
    "            for raiting in ratings:\n",
    "                if raiting.find_all('meta', itemprop=\"ratingCount\"):\n",
    "                    ratingCount = raiting.text.replace('\\n', '').strip().split(' ')[0].replace(',', '')\n",
    "                elif raiting.find_all('meta', itemprop=\"reviewCount\"):\n",
    "                    reviewCount = raiting.text.replace('\\n', '').strip().split(' ')[0].replace(',', '')\n",
    "        except:\n",
    "            ratingCount = \" \"\n",
    "            reviewCount = \" \"\n",
    "            \n",
    "        #extract the book title\n",
    "        try:\n",
    "            bookTitle = soup.find_all('h1')[0].contents[0].replace('\\n', '').strip()\n",
    "        except:\n",
    "            bookTitle = \" \"\n",
    "\n",
    "        #extract the book authors\n",
    "        try:\n",
    "            bookAuthors = soup.find_all('span', itemprop='name')[0].contents[0]\n",
    "        except:\n",
    "            bookAuthors = \" \"\n",
    "\n",
    "        #extract the book authors, we shoul FIX it.\n",
    "        try:\n",
    "            Plot = soup.find_all('div', id=\"description\")[0].contents[3].text\n",
    "            if detect(Plot) != \"en\":\n",
    "                Plot = \" \"\n",
    "        except:\n",
    "            try:\n",
    "                Plot = soup.find_all('div', id=\"description\")[0].contents[1].text\n",
    "                if detect(Plot) != \"en\":\n",
    "                    Plot = \" \"\n",
    "            except:\n",
    "                Plot = \" \"\n",
    "                \n",
    "\n",
    "        #extract the date\n",
    "        try:\n",
    "            date = soup.find_all('div', id=\"details\")[0].contents[3].text.replace('\\n', '').strip().split()\n",
    "            Published = date[1]+\" \"+date[2]+\" \"+date[3]\n",
    "        except:\n",
    "            Published = \" \"\n",
    "\n",
    "        #Rating Value\n",
    "        try:\n",
    "            ratingValue = soup.find('span', itemprop=\"ratingValue\").text.strip()\n",
    "        except:\n",
    "            ratingValue = \" \"\n",
    "\n",
    "        #Number of pages\n",
    "        try:\n",
    "            NumberofPages = soup.find('span', itemprop=\"numberOfPages\").text.split()[0]\n",
    "        except:\n",
    "            NumberofPages = \" \"\n",
    "\n",
    "        #Title series\n",
    "        try:\n",
    "            bookSeries = soup.find_all('a', href= re.compile(r'/series/*'))[0].contents[0].strip()\n",
    "        except:\n",
    "            bookSeries = \" \"\n",
    "            \n",
    "        #Places\n",
    "        try:\n",
    "            Setting = []\n",
    "            for places in soup.find_all('a', href= re.compile(r'/places/*')):\n",
    "                Setting.append(places.text)\n",
    "            Setting = \", \".join(Setting) if len(Setting)>=1 else \" \"\n",
    "        except:\n",
    "            Setting = \" \"\n",
    "\n",
    "        #list of characters\n",
    "        try:\n",
    "            Characters = []\n",
    "            for character in soup.find_all('a', href= re.compile(r'/characters/*') ):\n",
    "                Characters.append(character.text)\n",
    "            Characters = \", \".join(Characters) if len(Characters)>=1 else \" \"\n",
    "        except:\n",
    "            Characters = \" \"\n",
    "\n",
    "        #extract the Url\n",
    "        try:\n",
    "            Url = soup.find_all('link')[0][\"href\"]\n",
    "        except:\n",
    "            Url = \" \"\n",
    "\n",
    "        tsv_writer.writerow([bookTitle, bookSeries, bookAuthors, ratingValue, ratingCount, reviewCount, Plot, NumberofPages, Published, Characters, Setting, Url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"./html_pages\"\n",
    "\n",
    "filenames = os.listdir(path)\n",
    "for i in range(1, 101):\n",
    "    filenames = os.listdir(path + '/' + str(i))\n",
    "\n",
    "    for file in filenames:\n",
    "        with open(path + '/' + str(i) + './article_'+str(file.split(\"_\")[1].replace(\".html\", \"\"))+'.tsv', 'w', encoding=\"utf-8\", newline='') as out_file:\n",
    "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "            tsv_writer.writerow(['bookTitle', 'bookSeries', 'bookAuthors', 'ratingValue', \n",
    "                            'ratingCount', 'reviewCount', 'Plot', 'NumberofPages', 'Published',\n",
    "                            'Characters', 'Setting', 'Url'])\n",
    "            scrap_book(tsv_writer, path + '/' + str(i) + \"/\" + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./html_pages\"\n",
    "suffix = \".tsv\"\n",
    "filenames = os.listdir(path)\n",
    "data2 = pd.DataFrame()\n",
    "for i in range(1, 101):\n",
    "    filenames = os.listdir(path + '/' + str(i))\n",
    "    for file in filenames:\n",
    "        if file.endswith(suffix):\n",
    "            with open(path + '/' + str(i) + '/article_'+str(file.split(\"_\")[1]), 'r', encoding=\"utf-8\", newline='') as out_file:\n",
    "                    df = pd.read_csv(out_file,sep = \"\\t\")\n",
    "                    if  df.loc[0,\"Plot\"] != \" \" and df.loc[0,\"bookTitle\"] != \" \":\n",
    "                        data2 = pd.concat([data2,df])\n",
    "with open(\"output1-100.tsv\", \"w\", encoding=\"utf-8\", newline=\"\") as text_file: text_file.write(data2.to_csv(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ[2] Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('output1-100.tsv',sep=\",\")\n",
    "#df2 = pd.read_csv('NEWoutput2.tsv', sep=\"\\t\")\n",
    "#df3 = pd.read_csv('NEWoutput3.tsv', sep=\"\\t\")\n",
    "\n",
    "#complete_dataset = pd.concat([df1, df2, df3])\n",
    "#complete_dataset = complete_dataset.reset_index(drop=True) # reset index\n",
    "\n",
    "#complete_dataset = complete_dataset.drop_duplicates(subset=['bookTitle'])\n",
    "#complete_dataset = complete_dataset.dropna(subset=['Plot'])\n",
    "#complete_dataset = complete_dataset.reset_index(drop=True)\n",
    "#complete_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Cleaning data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEF Cleaning\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningDataset(df):\n",
    "    for i, row in df.iterrows():\n",
    "        tokenizer = RegexpTokenizer(\"[\\w']+\")  # import the tokenizer punctuation\n",
    "        \n",
    "        df.at[i, 'Plot'] = tokenizer.tokenize(df.at[i, 'Plot'].lower()) # remove the punctuation\n",
    "        df.at[i, 'Plot'] = [w for w in df.at[i, 'Plot'] if not w in set(stopwords.words('english'))]  # words in english to avoid few data for the cleaning data\n",
    "        df.at[i, 'Plot'] = [PorterStemmer().stem(word) for word in df.at[i, 'Plot']] # contesto\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabulary(df):\n",
    "    vocabulary = {}\n",
    "    for i, row in df.iterrows():\n",
    "        for word in df.at[i, 'Plot']:\n",
    "            if word in vocabulary.keys():\n",
    "                if \"document_\"+str(i) not in vocabulary[word]:\n",
    "                    vocabulary[word].append(\"document_\"+str(i))\n",
    "            else:\n",
    "                vocabulary[word] = [\"document_\"+str(i)]\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create and clean the dataset\n",
    "df = df1[1:20].copy()#complete_dataset.copy()\n",
    "df = cleaningDataset(df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = createVocabulary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create file .csv\n",
    "import csv\n",
    "\n",
    "with open('./vocabulary.tsv', 'w', encoding=\"utf-8\", newline='') as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "    tsv_writer.writerow([\"Word\", \"Term_id\", \"Document_List\"])\n",
    "    \n",
    "    list_of_words = vocabulary.keys()\n",
    "    for id, word in enumerate(list_of_words, 1):\n",
    "        term_id_i = \"term_id_\"+str(id)\n",
    "        tsv_writer.writerow([word, term_id_i, vocabulary[word]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ[2.1.2] Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanQuery(row):\n",
    "    row = row.split(\" \") # split the string query\n",
    "    for element in row:\n",
    "        tokenizer = RegexpTokenizer(\"[\\w']+\")  # import the tokenizer punctuation\n",
    "        element = tokenizer.tokenize(element.lower()) # remove the punctuation\n",
    "    \n",
    "    row = [w for w in row if not w in set(stopwords.words('english'))] # words in english to avoid few data for the cleaning data\n",
    "    \n",
    "    #row = [lemmatizer.lemmatize(word) for word in row]\n",
    "    row = [PorterStemmer().stem(word) for word in row]\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query string: door end\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Insert your query string: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['door', 'end']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanQString = cleanQuery(query)\n",
    "cleanQString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "open the dataset vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = pd.read_csv('vocabulary.tsv',sep=\"\\t\")\n",
    "vocabulary = vocabulary.drop_duplicates(subset=['Word'])\n",
    "vocabulary = vocabulary.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Term_id</th>\n",
       "      <th>Document_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>door</td>\n",
       "      <td>term_id_1</td>\n",
       "      <td>['document_1']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>end</td>\n",
       "      <td>term_id_2</td>\n",
       "      <td>['document_1', 'document_7']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>silent</td>\n",
       "      <td>term_id_3</td>\n",
       "      <td>['document_1']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>corridor</td>\n",
       "      <td>term_id_4</td>\n",
       "      <td>['document_1']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haunt</td>\n",
       "      <td>term_id_5</td>\n",
       "      <td>['document_1']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>protect</td>\n",
       "      <td>term_id_903</td>\n",
       "      <td>['document_19']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>babi</td>\n",
       "      <td>term_id_904</td>\n",
       "      <td>['document_19']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>fantin</td>\n",
       "      <td>term_id_905</td>\n",
       "      <td>['document_19']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>driven</td>\n",
       "      <td>term_id_906</td>\n",
       "      <td>['document_19']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>prostitut</td>\n",
       "      <td>term_id_907</td>\n",
       "      <td>['document_19']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>907 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word      Term_id                 Document_List\n",
       "0         door    term_id_1                ['document_1']\n",
       "1          end    term_id_2  ['document_1', 'document_7']\n",
       "2       silent    term_id_3                ['document_1']\n",
       "3     corridor    term_id_4                ['document_1']\n",
       "4        haunt    term_id_5                ['document_1']\n",
       "..         ...          ...                           ...\n",
       "902    protect  term_id_903               ['document_19']\n",
       "903       babi  term_id_904               ['document_19']\n",
       "904     fantin  term_id_905               ['document_19']\n",
       "905     driven  term_id_906               ['document_19']\n",
       "906  prostitut  term_id_907               ['document_19']\n",
       "\n",
       "[907 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the first inverted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_it_string(word):\n",
    "    for sym in [\"[\", \"]\", \"'\"]:\n",
    "        word = word.replace(sym, \"\")\n",
    "    word = word.split(\", \")\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_lst = {}\n",
    "for i, row in vocabulary.iterrows():\n",
    "    term = vocabulary.at[i, \"Term_id\"]\n",
    "    inv_lst[term] = make_it_string(vocabulary.at[i, \"Document_List\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inv_lst.csv', 'w') as f:\n",
    "    for key in inv_lst.keys():\n",
    "        f.write(\"%s:%s\\n\"%(key,inv_lst[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine, 1st version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the inv_lst\n",
    "reader = csv.reader(open('inv_lst.csv', 'r'), delimiter=\":\")\n",
    "inv_lst = {}\n",
    "for row in reader:\n",
    "    k, v = row\n",
    "    inv_lst[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in inv_lst:\n",
    "    inv_lst[key] = make_it_string(inv_lst[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find each term_id\n",
    "term_id = []\n",
    "for token in cleanQString:\n",
    "    term = vocabulary.loc[vocabulary[\"Word\"] == token, \"Term_id\"].values[0]\n",
    "    term_id.append(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_list = []\n",
    "for term in term_id:\n",
    "    if not intersection_list:\n",
    "        intersection_list = inv_lst[term]\n",
    "    else:\n",
    "        intersection_list = set(intersection_list).intersection(set(inv_lst[term]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(columns=['bookTitle', 'Plot', 'Url'])\n",
    "\n",
    "for row in intersection_list:\n",
    "    i = int(row.split(\"_\")[1])\n",
    "    \n",
    "    #append row to the dataframe\n",
    "    new_row = {'bookTitle': df.loc[i, \"bookTitle\"], 'Plot': df1.loc[i, \"Plot\"], 'Url': df.loc[i, \"Url\"]}\n",
    "    new_df = new_df.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>There is a door at the end of a silent corrido...</td>\n",
       "      <td>https://www.goodreads.com/book/show/2.Harry_Po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   bookTitle  \\\n",
       "0  Harry Potter and the Order of the Phoenix   \n",
       "\n",
       "                                                Plot  \\\n",
       "0  There is a door at the end of a silent corrido...   \n",
       "\n",
       "                                                 Url  \n",
       "0  https://www.goodreads.com/book/show/2.Harry_Po...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ[2.2] Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Term_id</th>\n",
       "      <th>Document_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>door</td>\n",
       "      <td>term_id_1</td>\n",
       "      <td>['document_1']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>end</td>\n",
       "      <td>term_id_2</td>\n",
       "      <td>['document_1', 'document_7']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>silent</td>\n",
       "      <td>term_id_3</td>\n",
       "      <td>['document_1']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>corridor</td>\n",
       "      <td>term_id_4</td>\n",
       "      <td>['document_1']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haunt</td>\n",
       "      <td>term_id_5</td>\n",
       "      <td>['document_1']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>protect</td>\n",
       "      <td>term_id_903</td>\n",
       "      <td>['document_19']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>babi</td>\n",
       "      <td>term_id_904</td>\n",
       "      <td>['document_19']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>fantin</td>\n",
       "      <td>term_id_905</td>\n",
       "      <td>['document_19']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>driven</td>\n",
       "      <td>term_id_906</td>\n",
       "      <td>['document_19']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>prostitut</td>\n",
       "      <td>term_id_907</td>\n",
       "      <td>['document_19']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>907 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word      Term_id                 Document_List\n",
       "0         door    term_id_1                ['document_1']\n",
       "1          end    term_id_2  ['document_1', 'document_7']\n",
       "2       silent    term_id_3                ['document_1']\n",
       "3     corridor    term_id_4                ['document_1']\n",
       "4        haunt    term_id_5                ['document_1']\n",
       "..         ...          ...                           ...\n",
       "902    protect  term_id_903               ['document_19']\n",
       "903       babi  term_id_904               ['document_19']\n",
       "904     fantin  term_id_905               ['document_19']\n",
       "905     driven  term_id_906               ['document_19']\n",
       "906  prostitut  term_id_907               ['document_19']\n",
       "\n",
       "[907 rows x 3 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the inv_lst\n",
    "reader = csv.reader(open('inv_lst.csv', 'r'), delimiter=\":\")\n",
    "inv_lst = {}\n",
    "for row in reader:\n",
    "    k, v = row\n",
    "    inv_lst[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in inv_lst:\n",
    "    inv_lst[key] = make_it_string(inv_lst[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "inv_lst2 = {}\n",
    "for i, row in vocabulary.iterrows():\n",
    "    lst_doc = make_it_string(vocabulary.at[i, 'Document_List'])\n",
    "    result = []\n",
    "    for doc in lst_doc:\n",
    "        number_doc = int(doc.split(\"_\")[1])\n",
    "\n",
    "        interested_row = df.at[number_doc, \"Plot\"]\n",
    "        \n",
    "        interested_word = vocabulary.at[i, \"Word\"] #i-th word\n",
    "        \n",
    "        tf = interested_row.count(interested_word) / len(interested_row)\n",
    "        \n",
    "        idf = math.log(len(df)/len(lst_doc))\n",
    "\n",
    "        tf_idf = round(tf * idf, 3)\n",
    "       \n",
    "        result.append((doc, tf_idf))\n",
    "        \n",
    "    inv_lst2[vocabulary.at[i, \"Term_id\"]] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ[2.2.2] SEARCH ENGINE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_it_string(word):\n",
    "    for sym in [\"[\", \"]\", \"'\", \"(\", \")\"]:\n",
    "        word = word.replace(sym, \"\")\n",
    "    word = word.split(\", \")\n",
    "    \n",
    "    return list(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query string: door end\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Insert your query string: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['door', 'end']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanQString = cleanQuery(query)\n",
    "cleanQString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find each term_id\n",
    "def returnTermId(token):\n",
    "    return vocabulary.loc[vocabulary[\"Word\"] == token, \"Term_id\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for i, row in df.iterrows():\n",
    "    print(i)\n",
    "    tokens = {}\n",
    "    for token in df.at[i, \"Plot\"]:\n",
    "        if token != \"nan\": #nlp --> wrong the process\n",
    "            tuple_list_values = inv_lst2[returnTermId(token)]\n",
    "        \n",
    "            for x in tuple_list_values:\n",
    "                if int(x[0].split(\"_\")[1]) == i:\n",
    "                    tokens[token] = x[1]  \n",
    "                    break\n",
    "        \n",
    "    documents.append(tokens)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "top_k_documents = [] #similarity\n",
    "for i, row in df.iterrows():\n",
    "    card_d_i = 1 / math.sqrt( sum(documents[i-1].values()) )\n",
    "    # card_q = 1 / math.sqrt(len(cleanQString)) # we suppose this costant to be equal to each document and so we obtain the same order for the cosine similarity\n",
    "    \n",
    "    somma = 0\n",
    "    for token in cleanQString:\n",
    "        try:\n",
    "            somma += documents[i-1][token]\n",
    "        except:\n",
    "            somma += 0\n",
    "            \n",
    "    cosine_similarity = card_d_i * somma\n",
    "        \n",
    "    top_k_documents.append([round(cosine_similarity, 2), \"document_\"+str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq \n",
    "# using heapify to convert list into heap \n",
    "heapq.heapify(top_k_documents) \n",
    "show_top_k_documents = (heapq.nlargest(20, top_k_documents)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(columns=['bookTitle', 'Plot', 'Url', 'Similarity'])\n",
    "\n",
    "for row in show_top_k_documents:\n",
    "    i = int(row[1].split(\"_\")[1])\n",
    "    \n",
    "    #append row to the dataframe\n",
    "    new_row = {'bookTitle': df.loc[i, \"bookTitle\"], 'Plot': df1.loc[i, \"Plot\"], 'Url': df.loc[i, \"Url\"], 'Similarity': row[0]}\n",
    "    new_df = new_df.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>There is a door at the end of a silent corrido...</td>\n",
       "      <td>https://www.goodreads.com/book/show/2.Harry_Po...</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Chronicles of Narnia</td>\n",
       "      <td>Journeys to the end of the world, fantastic cr...</td>\n",
       "      <td>https://www.goodreads.com/book/show/11127.The_...</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Fault in Our Stars</td>\n",
       "      <td>Despite the tumor-shrinking medical miracle th...</td>\n",
       "      <td>https://www.goodreads.com/book/show/11870085-t...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>J.R.R. Tolkien 4-Book Boxed Set: The Hobbit an...</td>\n",
       "      <td>This four-volume, boxed set contains J.R.R. To...</td>\n",
       "      <td>https://www.goodreads.com/book/show/30.J_R_R_T...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Animal Farm</td>\n",
       "      <td>Librarian's note: There is an Alternate Cover ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/170448.Ani...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Book Thief</td>\n",
       "      <td>Librarian's note: An alternate cover edition c...</td>\n",
       "      <td>https://www.goodreads.com/book/show/19063.The_...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Twilight</td>\n",
       "      <td>About three things I was absolutely positive.F...</td>\n",
       "      <td>https://www.goodreads.com/book/show/41865.Twil...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>Alternate cover edition of ISBN 9780679783268S...</td>\n",
       "      <td>https://www.goodreads.com/book/show/1885.Pride...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>The unforgettable novel of a childhood in a sl...</td>\n",
       "      <td>https://www.goodreads.com/book/show/2657.To_Ki...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Les Misérables</td>\n",
       "      <td>Victor Hugo's tale of injustice, heroism and l...</td>\n",
       "      <td>https://www.goodreads.com/book/show/24280.Les_...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Jane Eyre</td>\n",
       "      <td>Orphaned as a child, Jane has felt an outcast ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/10210.Jane...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Alice's Adventures in Wonderland / Through the...</td>\n",
       "      <td>\"I can't explain myself, I'm afraid, sir,\" sai...</td>\n",
       "      <td>https://www.goodreads.com/book/show/24213.Alic...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Memoirs of a Geisha</td>\n",
       "      <td>A literary sensation and runaway bestseller, t...</td>\n",
       "      <td>https://www.goodreads.com/book/show/929.Memoir...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Picture of Dorian Gray</td>\n",
       "      <td>Written in his distinctively dazzling manner, ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/5297.The_P...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The Da Vinci Code</td>\n",
       "      <td>ISBN 9780307277671 moved to this edition.While...</td>\n",
       "      <td>https://www.goodreads.com/book/show/968.The_Da...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Wuthering Heights</td>\n",
       "      <td>You can find the redesigned cover of this edit...</td>\n",
       "      <td>https://www.goodreads.com/book/show/6185.Wuthe...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The Giving Tree</td>\n",
       "      <td>\"Once there was a tree...and she loved a littl...</td>\n",
       "      <td>https://www.goodreads.com/book/show/370493.The...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy</td>\n",
       "      <td>Seconds before the Earth is demolished to make...</td>\n",
       "      <td>https://www.goodreads.com/book/show/386162.The...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Gone with the Wind</td>\n",
       "      <td>Scarlett O'Hara, the beautiful, spoiled daught...</td>\n",
       "      <td>https://www.goodreads.com/book/show/18405.Gone...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            bookTitle  \\\n",
       "0           Harry Potter and the Order of the Phoenix   \n",
       "1                            The Chronicles of Narnia   \n",
       "2                              The Fault in Our Stars   \n",
       "3   J.R.R. Tolkien 4-Book Boxed Set: The Hobbit an...   \n",
       "4                                         Animal Farm   \n",
       "5                                      The Book Thief   \n",
       "6                                            Twilight   \n",
       "7                                 Pride and Prejudice   \n",
       "8                               To Kill a Mockingbird   \n",
       "9                                      Les Misérables   \n",
       "10                                          Jane Eyre   \n",
       "11  Alice's Adventures in Wonderland / Through the...   \n",
       "12                                Memoirs of a Geisha   \n",
       "13                         The Picture of Dorian Gray   \n",
       "14                                  The Da Vinci Code   \n",
       "15                                  Wuthering Heights   \n",
       "16                                    The Giving Tree   \n",
       "17               The Hitchhiker's Guide to the Galaxy   \n",
       "18                                 Gone with the Wind   \n",
       "\n",
       "                                                 Plot  \\\n",
       "0   There is a door at the end of a silent corrido...   \n",
       "1   Journeys to the end of the world, fantastic cr...   \n",
       "2   Despite the tumor-shrinking medical miracle th...   \n",
       "3   This four-volume, boxed set contains J.R.R. To...   \n",
       "4   Librarian's note: There is an Alternate Cover ...   \n",
       "5   Librarian's note: An alternate cover edition c...   \n",
       "6   About three things I was absolutely positive.F...   \n",
       "7   Alternate cover edition of ISBN 9780679783268S...   \n",
       "8   The unforgettable novel of a childhood in a sl...   \n",
       "9   Victor Hugo's tale of injustice, heroism and l...   \n",
       "10  Orphaned as a child, Jane has felt an outcast ...   \n",
       "11  \"I can't explain myself, I'm afraid, sir,\" sai...   \n",
       "12  A literary sensation and runaway bestseller, t...   \n",
       "13  Written in his distinctively dazzling manner, ...   \n",
       "14  ISBN 9780307277671 moved to this edition.While...   \n",
       "15  You can find the redesigned cover of this edit...   \n",
       "16  \"Once there was a tree...and she loved a littl...   \n",
       "17  Seconds before the Earth is demolished to make...   \n",
       "18  Scarlett O'Hara, the beautiful, spoiled daught...   \n",
       "\n",
       "                                                  Url  Similarity  \n",
       "0   https://www.goodreads.com/book/show/2.Harry_Po...        0.04  \n",
       "1   https://www.goodreads.com/book/show/11127.The_...        0.01  \n",
       "2   https://www.goodreads.com/book/show/11870085-t...        0.00  \n",
       "3   https://www.goodreads.com/book/show/30.J_R_R_T...        0.00  \n",
       "4   https://www.goodreads.com/book/show/170448.Ani...        0.00  \n",
       "5   https://www.goodreads.com/book/show/19063.The_...        0.00  \n",
       "6   https://www.goodreads.com/book/show/41865.Twil...        0.00  \n",
       "7   https://www.goodreads.com/book/show/1885.Pride...        0.00  \n",
       "8   https://www.goodreads.com/book/show/2657.To_Ki...        0.00  \n",
       "9   https://www.goodreads.com/book/show/24280.Les_...        0.00  \n",
       "10  https://www.goodreads.com/book/show/10210.Jane...        0.00  \n",
       "11  https://www.goodreads.com/book/show/24213.Alic...        0.00  \n",
       "12  https://www.goodreads.com/book/show/929.Memoir...        0.00  \n",
       "13  https://www.goodreads.com/book/show/5297.The_P...        0.00  \n",
       "14  https://www.goodreads.com/book/show/968.The_Da...        0.00  \n",
       "15  https://www.goodreads.com/book/show/6185.Wuthe...        0.00  \n",
       "16  https://www.goodreads.com/book/show/370493.The...        0.00  \n",
       "17  https://www.goodreads.com/book/show/386162.The...        0.00  \n",
       "18  https://www.goodreads.com/book/show/18405.Gone...        0.00  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ[3.] Define a new score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query string: Let's do the thing videogame\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Insert your query string: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"let'\", 'thing', 'videogam']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanQString = cleanQuery(query)\n",
    "cleanQString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the inv_lst\n",
    "reader = csv.reader(open('inv_lst.csv', 'r'), delimiter=\":\")\n",
    "inv_lst = {}\n",
    "for row in reader:\n",
    "    k, v = row\n",
    "    inv_lst[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in inv_lst:\n",
    "    inv_lst[key] = make_it_string(inv_lst[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find each term_id\n",
    "term_id = []\n",
    "for token in cleanQString:\n",
    "    term = vocabulary.loc[vocabulary[\"Word\"] == token, \"Term_id\"].values[0]\n",
    "    term_id.append(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_list = []\n",
    "for term in term_id:\n",
    "    if not intersection_list:\n",
    "        intersection_list = inv_lst[term]\n",
    "    else:\n",
    "        intersection_list = set(intersection_list).intersection(set(inv_lst[term]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(columns=['bookTitle', 'Plot', 'Url'])\n",
    "\n",
    "for row in intersection_list:\n",
    "    i = int(row.split(\"_\")[1])\n",
    "    \n",
    "    #append row to the dataframe\n",
    "    new_row = {'bookTitle': df.loc[i, \"bookTitle\"], 'Plot': df1.loc[i, \"Plot\"], 'Url': df.loc[i, \"Url\"]}\n",
    "    new_df = new_df.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [bookTitle, Plot, Url]\n",
       "Index: []"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import lxml #is a parser for HTMl\n",
    "import webscraping\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "#from selenium import webdriver\n",
    "from langdetect import detect\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shoul generalize these functions... in webscraping.py!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **[RQ1.1] Get the list of books**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start to scrape the three pages to extract the URLs for each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "UrlsFiles = open(\"urlpages.txt\", \"w\")\n",
    "\n",
    "for i in range(1, 101):\n",
    "    url = \"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"+str(i)\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, features='lxml')\n",
    "    for a in soup.find_all('a', class_=\"bookTitle\"):\n",
    "        UrlsFiles.write(a.get('href')+'\\n')\n",
    "\n",
    "UrlsFiles.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RQ[1.2] Crawl books**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "headpart = \"https://www.goodreads.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = \"htmlpages\" #Directory  \n",
    "parent_dir = \"./\" #Parent Directory path \n",
    "pathAncestor = os.path.join(parent_dir, direct) #Path\n",
    "os.mkdir(pathAncestor) #create the folder in the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,101):\n",
    "    os.makedirs(os.path.join(pathAncestor, 'page ' + str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "UrlsFiles = open(\"urlpages.txt\", \"r\")\n",
    "\n",
    "counter_pages = 0\n",
    "counter_html = 0\n",
    "for x in UrlsFiles:\n",
    "    if counter_html % 100 == 0:\n",
    "        counter_pages = counter_pages+1\n",
    "        \n",
    "    counter_html = counter_html + 1\n",
    "    \n",
    "    subdirectory = pathAncestor + \"/page \" + str(counter_pages)\n",
    "    article_name = \"/article_\"+str(counter_html)+\".html\"\n",
    "    \n",
    "    complete_path = subdirectory + article_name\n",
    "    with open(complete_path, \"wb\") as ip_file:\n",
    "        link = headpart + x\n",
    "        try:\n",
    "            page = requests.get(link)\n",
    "        except:\n",
    "            with open(\"failureRequest.txt\", \"wb\") as err_file:\n",
    "                err_file.write(link)\n",
    "                err_file.close()\n",
    "        \n",
    "        soup = BeautifulSoup(page.text, features='lxml')\n",
    "        \n",
    "        ip_file.write(soup.encode('utf-8'))\n",
    "        ip_file.close()\n",
    "\n",
    "UrlsFiles.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RQ[1.3] Parse downloaded pages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_book(tsv_writer, article):\n",
    "    global bookTitle, bookSeries, bookAuthors, ratingValue, ratingCount, reviewCount, Plot, NumberofPages, Published, Characters, Setting, Url;\n",
    "    \n",
    "    with open(article, 'r', encoding=\"utf-8\") as out_file:\n",
    "        contents = out_file.read()\n",
    "        soup = BeautifulSoup(contents, features=\"lxml\") #parse the text\n",
    "\n",
    "        #extract rating and review count\n",
    "        try:\n",
    "            ratings = soup.find_all('a', href=\"#other_reviews\") #search the ratings in its\n",
    "            rating_count = -1\n",
    "            rating = -1\n",
    "            for raiting in ratings:\n",
    "                if raiting.find_all('meta', itemprop=\"ratingCount\"):\n",
    "                    ratingCount = raiting.text.replace('\\n', '').strip().split(' ')[0].replace(',', '')\n",
    "                elif raiting.find_all('meta', itemprop=\"reviewCount\"):\n",
    "                    reviewCount = raiting.text.replace('\\n', '').strip().split(' ')[0].replace(',', '')\n",
    "        except:\n",
    "            ratingCount = \" \"\n",
    "            reviewCount = \" \"\n",
    "            \n",
    "        #extract the book title\n",
    "        try:\n",
    "            bookTitle = soup.find_all('h1')[0].contents[0].replace('\\n', '').strip()\n",
    "        except:\n",
    "            bookTitle = \" \"\n",
    "\n",
    "        #extract the book authors\n",
    "        try:\n",
    "            bookAuthors = soup.find_all('span', itemprop='name')[0].contents[0]\n",
    "        except:\n",
    "            bookAuthors = \" \"\n",
    "\n",
    "        #extract the book authors, we shoul FIX it.\n",
    "        try:\n",
    "            Plot = soup.find_all('div', id=\"description\")[0].contents[3].text\n",
    "            if detect(Plot) != \"en\":\n",
    "                Plot = \" \"\n",
    "        except:\n",
    "            try:\n",
    "                Plot = soup.find_all('div', id=\"description\")[0].contents[1].text\n",
    "                if detect(Plot) != \"en\":\n",
    "                    Plot = \" \"\n",
    "            except:\n",
    "                Plot = \" \"\n",
    "                \n",
    "\n",
    "        #extract the date\n",
    "        try:\n",
    "            date = soup.find_all('div', id=\"details\")[0].contents[3].text.replace('\\n', '').strip().split()\n",
    "            Published = date[1]+\" \"+date[2]+\" \"+date[3]\n",
    "        except:\n",
    "            Published = \" \"\n",
    "\n",
    "        #Rating Value\n",
    "        try:\n",
    "            ratingValue = soup.find('span', itemprop=\"ratingValue\").text.strip()\n",
    "        except:\n",
    "            ratingValue = \" \"\n",
    "\n",
    "        #Number of pages\n",
    "        try:\n",
    "            NumberofPages = soup.find('span', itemprop=\"numberOfPages\").text.split()[0]\n",
    "        except:\n",
    "            NumberofPages = \" \"\n",
    "\n",
    "        #Title series\n",
    "        try:\n",
    "            bookSeries = soup.find_all('a', href= re.compile(r'/series/*'))[0].contents[0].strip()\n",
    "        except:\n",
    "            bookSeries = \" \"\n",
    "            \n",
    "        #Places\n",
    "        try:\n",
    "            Setting = []\n",
    "            for places in soup.find_all('a', href= re.compile(r'/places/*')):\n",
    "                Setting.append(places.text)\n",
    "            Setting = \", \".join(Setting) if len(Setting)>=1 else \" \"\n",
    "        except:\n",
    "            Setting = \" \"\n",
    "\n",
    "        #list of characters\n",
    "        try:\n",
    "            Characters = []\n",
    "            for character in soup.find_all('a', href= re.compile(r'/characters/*') ):\n",
    "                Characters.append(character.text)\n",
    "            Characters = \", \".join(Characters) if len(Characters)>=1 else \" \"\n",
    "        except:\n",
    "            Characters = \" \"\n",
    "\n",
    "        #extract the Url\n",
    "        try:\n",
    "            Url = soup.find_all('link')[0][\"href\"]\n",
    "        except:\n",
    "            Url = \" \"\n",
    "\n",
    "        tsv_writer.writerow([bookTitle, bookSeries, bookAuthors, ratingValue, ratingCount, reviewCount, Plot, NumberofPages, Published, Characters, Setting, Url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"./html_pages\"\n",
    "\n",
    "filenames = os.listdir(path)\n",
    "for i in range(1, 101):\n",
    "    filenames = os.listdir(path + '/' + str(i))\n",
    "\n",
    "    for file in filenames:\n",
    "        with open(path + '/' + str(i) + './article_'+str(file.split(\"_\")[1].replace(\".html\", \"\"))+'.tsv', 'w', encoding=\"utf-8\", newline='') as out_file:\n",
    "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "            tsv_writer.writerow(['bookTitle', 'bookSeries', 'bookAuthors', 'ratingValue', \n",
    "                            'ratingCount', 'reviewCount', 'Plot', 'NumberofPages', 'Published',\n",
    "                            'Characters', 'Setting', 'Url'])\n",
    "            scrap_book(tsv_writer, path + '/' + str(i) + \"/\" + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ[2] Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('NEWoutput1.tsv',sep=\"\\t\")\n",
    "#df2 = pd.read_csv('NEWoutput2.tsv', sep=\"\\t\")\n",
    "#df3 = pd.read_csv('NEWoutput3.tsv', sep=\"\\t\")\n",
    "\n",
    "#complete_dataset = pd.concat([df1, df2, df3])\n",
    "#complete_dataset = complete_dataset.reset_index(drop=True) # reset index\n",
    "\n",
    "#complete_dataset = complete_dataset.drop_duplicates(subset=['bookTitle'])\n",
    "#complete_dataset = complete_dataset.dropna(subset=['Plot'])\n",
    "#complete_dataset = complete_dataset.reset_index(drop=True)\n",
    "#complete_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Cleaning data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEF Cleaning\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningDataset(df):\n",
    "    for i, row in df.iterrows():\n",
    "        tokenizer = RegexpTokenizer(\"[\\w']+\")  # import the tokenizer punctuation\n",
    "        \n",
    "        df.at[i, 'Plot'] = tokenizer.tokenize(df.at[i, 'Plot'].lower()) # remove the punctuation\n",
    "        df.at[i, 'Plot'] = [w for w in df.at[i, 'Plot'] if not w in set(stopwords.words('english'))]  # words in english to avoid few data for the cleaning data\n",
    "        df.at[i, 'Plot'] = [PorterStemmer().stem(word) for word in df.at[i, 'Plot']] # contesto\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabulary(df):\n",
    "    vocabulary = {}\n",
    "    for i, row in df.iterrows():\n",
    "        for word in df.at[i, 'Plot']:\n",
    "            if word in vocabulary.keys():\n",
    "                if \"document_\"+str(i) not in vocabulary[word]:\n",
    "                    vocabulary[word].append(\"document_\"+str(i))\n",
    "            else:\n",
    "                vocabulary[word] = [\"document_\"+str(i)]\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create and clean the dataset\n",
    "df = df1[0:50].copy()#complete_dataset.copy()\n",
    "df = cleaningDataset(df)\n",
    "\n",
    "vocabulary = createVocabulary(df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create file .csv\n",
    "import csv\n",
    "\n",
    "with open('./vocabulary.tsv', 'w', encoding=\"utf-8\", newline='') as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "    tsv_writer.writerow([\"Word\", \"Term_id\", \"Document_List\"])\n",
    "    \n",
    "    list_of_words = vocabulary.keys()\n",
    "    for id, word in enumerate(list_of_words, 1):\n",
    "        term_id_i = \"term_id_\"+str(id)\n",
    "        tsv_writer.writerow([word, term_id_i, vocabulary[word]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ[2.1.2] Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanQuery(row):\n",
    "    row = row.split(\" \") # split the string query\n",
    "    for element in row:\n",
    "        tokenizer = RegexpTokenizer(\"[\\w']+\")  # import the tokenizer punctuation\n",
    "        element = tokenizer.tokenize(element.lower()) # remove the punctuation\n",
    "    \n",
    "    row = [w for w in row if not w in set(stopwords.words('english'))] # words in english to avoid few data for the cleaning data\n",
    "    \n",
    "    #row = [lemmatizer.lemmatize(word) for word in row]\n",
    "    row = [PorterStemmer().stem(word) for word in row]\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query string: Survival games\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Insert your query string: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['surviv', 'game']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanQString = cleanQuery(query)\n",
    "cleanQString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "open the dataset vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = pd.read_csv('vocabulary.tsv',sep=\"\\t\")\n",
    "vocabulary = vocabulary.drop_duplicates(subset=['Word'])\n",
    "vocabulary = vocabulary.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Term_id</th>\n",
       "      <th>Document_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>could</td>\n",
       "      <td>term_id_1</td>\n",
       "      <td>['document_0', 'document_26', 'document_38']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>surviv</td>\n",
       "      <td>term_id_2</td>\n",
       "      <td>['document_0', 'document_26', 'document_40', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wild</td>\n",
       "      <td>term_id_3</td>\n",
       "      <td>['document_0', 'document_8', 'document_19', 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>everi</td>\n",
       "      <td>term_id_4</td>\n",
       "      <td>['document_0', 'document_3', 'document_5', 'do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one</td>\n",
       "      <td>term_id_5</td>\n",
       "      <td>['document_0', 'document_9', 'document_14', 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>manheim</td>\n",
       "      <td>term_id_1989</td>\n",
       "      <td>['document_49']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>1996</td>\n",
       "      <td>term_id_1990</td>\n",
       "      <td>['document_49']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>access</td>\n",
       "      <td>term_id_1991</td>\n",
       "      <td>['document_49']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>poem</td>\n",
       "      <td>term_id_1992</td>\n",
       "      <td>['document_49']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>vers</td>\n",
       "      <td>term_id_1993</td>\n",
       "      <td>['document_49']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1993 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word       Term_id                                      Document_List\n",
       "0       could     term_id_1       ['document_0', 'document_26', 'document_38']\n",
       "1      surviv     term_id_2  ['document_0', 'document_26', 'document_40', '...\n",
       "2        wild     term_id_3  ['document_0', 'document_8', 'document_19', 'd...\n",
       "3       everi     term_id_4  ['document_0', 'document_3', 'document_5', 'do...\n",
       "4         one     term_id_5  ['document_0', 'document_9', 'document_14', 'd...\n",
       "...       ...           ...                                                ...\n",
       "1988  manheim  term_id_1989                                    ['document_49']\n",
       "1989     1996  term_id_1990                                    ['document_49']\n",
       "1990   access  term_id_1991                                    ['document_49']\n",
       "1991     poem  term_id_1992                                    ['document_49']\n",
       "1992     vers  term_id_1993                                    ['document_49']\n",
       "\n",
       "[1993 rows x 3 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the first inverted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_it_string(word):\n",
    "    for sym in [\"[\", \"]\", \"'\"]:\n",
    "        word = word.replace(sym, \"\")\n",
    "    word = word.split(\", \")\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_lst = {}\n",
    "for i, row in vocabulary.iterrows():\n",
    "    term = vocabulary.at[i, \"Term_id\"]\n",
    "    inv_lst[term] = make_it_string(vocabulary.at[i, \"Document_List\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inv_lst.csv', 'w') as f:\n",
    "    for key in inv_lst.keys():\n",
    "        f.write(\"%s:%s\\n\"%(key,inv_lst[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inv_lst.set_index('Word', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term_id</th>\n",
       "      <th>Document_List</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>could</th>\n",
       "      <td>term_id_1</td>\n",
       "      <td>['document_1', 'document_27', 'document_39', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surviv</th>\n",
       "      <td>term_id_2</td>\n",
       "      <td>['document_1', 'document_27', 'document_41', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wild</th>\n",
       "      <td>term_id_3</td>\n",
       "      <td>['document_1', 'document_9', 'document_20', 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>everi</th>\n",
       "      <td>term_id_4</td>\n",
       "      <td>['document_1', 'document_4', 'document_6', 'do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>term_id_5</td>\n",
       "      <td>['document_1', 'document_10', 'document_15', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Term_id                                      Document_List\n",
       "Word                                                                \n",
       "could   term_id_1  ['document_1', 'document_27', 'document_39', '...\n",
       "surviv  term_id_2  ['document_1', 'document_27', 'document_41', '...\n",
       "wild    term_id_3  ['document_1', 'document_9', 'document_20', 'd...\n",
       "everi   term_id_4  ['document_1', 'document_4', 'document_6', 'do...\n",
       "one     term_id_5  ['document_1', 'document_10', 'document_15', '..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inv_lst.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine, 1st version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the inv_lst\n",
    "reader = csv.reader(open('inv_lst.csv', 'r'), delimiter=\":\")\n",
    "inv_lst = {}\n",
    "for row in reader:\n",
    "    k, v = row\n",
    "    inv_lst[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in inv_lst:\n",
    "    inv_lst[key] = make_it_string(inv_lst[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find each term_id\n",
    "term_id = []\n",
    "for token in cleanQString:\n",
    "    term = vocabulary.loc[vocabulary[\"Word\"] == token, \"Term_id\"].values[0]\n",
    "    term_id.append(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document_0', 'document_26', 'document_38']\n"
     ]
    }
   ],
   "source": [
    "intersection_list = []\n",
    "for term in term_id:\n",
    "    if not intersection_list:\n",
    "        intersection_list = inv_lst[term]\n",
    "    else:\n",
    "        intersection_list = set(intersection_list).intersection(set(inv_lst[term]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(columns=['bookTitle', 'Plot', 'Url'])\n",
    "\n",
    "for row in intersection_list:\n",
    "    i = int(row.split(\"_\")[1])\n",
    "    \n",
    "    #append row to the dataframe\n",
    "    new_row = {'bookTitle': df.loc[i, \"bookTitle\"], 'Plot': df1.loc[i, \"Plot\"], 'Url': df.loc[i, \"Url\"]}\n",
    "    new_df = new_df.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>Could you survive on your own in the wild, wit...</td>\n",
       "      <td>https://www.goodreads.com/book/show/2767052-th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          bookTitle                                               Plot  \\\n",
       "0  The Hunger Games  Could you survive on your own in the wild, wit...   \n",
       "\n",
       "                                                 Url  \n",
       "0  https://www.goodreads.com/book/show/2767052-th...  "
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ[2.2] Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Term_id</th>\n",
       "      <th>Document_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>could</td>\n",
       "      <td>term_id_1</td>\n",
       "      <td>['document_0', 'document_26', 'document_38']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>surviv</td>\n",
       "      <td>term_id_2</td>\n",
       "      <td>['document_0', 'document_26', 'document_40', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wild</td>\n",
       "      <td>term_id_3</td>\n",
       "      <td>['document_0', 'document_8', 'document_19', 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>everi</td>\n",
       "      <td>term_id_4</td>\n",
       "      <td>['document_0', 'document_3', 'document_5', 'do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one</td>\n",
       "      <td>term_id_5</td>\n",
       "      <td>['document_0', 'document_9', 'document_14', 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>manheim</td>\n",
       "      <td>term_id_1989</td>\n",
       "      <td>['document_49']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>1996</td>\n",
       "      <td>term_id_1990</td>\n",
       "      <td>['document_49']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>access</td>\n",
       "      <td>term_id_1991</td>\n",
       "      <td>['document_49']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>poem</td>\n",
       "      <td>term_id_1992</td>\n",
       "      <td>['document_49']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>vers</td>\n",
       "      <td>term_id_1993</td>\n",
       "      <td>['document_49']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1993 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word       Term_id                                      Document_List\n",
       "0       could     term_id_1       ['document_0', 'document_26', 'document_38']\n",
       "1      surviv     term_id_2  ['document_0', 'document_26', 'document_40', '...\n",
       "2        wild     term_id_3  ['document_0', 'document_8', 'document_19', 'd...\n",
       "3       everi     term_id_4  ['document_0', 'document_3', 'document_5', 'do...\n",
       "4         one     term_id_5  ['document_0', 'document_9', 'document_14', 'd...\n",
       "...       ...           ...                                                ...\n",
       "1988  manheim  term_id_1989                                    ['document_49']\n",
       "1989     1996  term_id_1990                                    ['document_49']\n",
       "1990   access  term_id_1991                                    ['document_49']\n",
       "1991     poem  term_id_1992                                    ['document_49']\n",
       "1992     vers  term_id_1993                                    ['document_49']\n",
       "\n",
       "[1993 rows x 3 columns]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the inv_lst\n",
    "reader = csv.reader(open('inv_lst.csv', 'r'), delimiter=\":\")\n",
    "inv_lst = {}\n",
    "for row in reader:\n",
    "    k, v = row\n",
    "    inv_lst[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in inv_lst:\n",
    "    inv_lst[key] = make_it_string(inv_lst[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "inv_lst2 = {}\n",
    "for i, row in vocabulary.iterrows():\n",
    "    lst_doc = make_it_string(vocabulary.at[i, 'Document_List'])\n",
    "    result = []\n",
    "    for doc in lst_doc:\n",
    "        number_doc = int(doc.split(\"_\")[1])\n",
    "\n",
    "        interested_row = df.at[number_doc, \"Plot\"]\n",
    "        \n",
    "        interested_word = vocabulary.at[i, \"Word\"] #i-th word\n",
    "        \n",
    "        tf = interested_row.count(interested_word) / len(interested_row)\n",
    "        \n",
    "        idf = math.log(len(df)/len(lst_doc))\n",
    "\n",
    "        tf_idf = round(tf * idf, 3)\n",
    "       \n",
    "        result.append((doc, tf_idf))\n",
    "        \n",
    "    inv_lst2[vocabulary.at[i, \"Term_id\"]] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ[2.2.2] SEARCH ENGINE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query string: could wild\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Insert your query string: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['could', 'wild']"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanQString = cleanQuery(query)\n",
    "cleanQString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_it_string(word):\n",
    "    for sym in [\"[\", \"]\", \"'\", \"(\", \")\"]:\n",
    "        word = word.replace(sym, \"\")\n",
    "    word = word.split(\", \")\n",
    "    \n",
    "    return list(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find each term_id\n",
    "def returnTermId(token):\n",
    "    return vocabulary.loc[vocabulary[\"Word\"] == token, \"Term_id\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for i, row in df.iterrows():\n",
    "    tokens = {}\n",
    "    for token in df.at[i, \"Plot\"]:\n",
    "        tuple_list_values = inv_lst2[returnTermId(token)]\n",
    "        \n",
    "        for x in tuple_list_values:\n",
    "            if int(x[0].split(\"_\")[1]) == i:\n",
    "                tokens[token] = x[1]  \n",
    "                break\n",
    "        \n",
    "    documents.append(tokens)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "top_k_documents = [] #similarity\n",
    "for i, row in df.iterrows():\n",
    "    card_d_i = 1 / math.sqrt( sum(documents[i].values()) )\n",
    "    card_q = 1 / math.sqrt(len(cleanQString))\n",
    "    \n",
    "    somma = 0\n",
    "    for token in cleanQString:\n",
    "        try:\n",
    "            somma += documents[i][token]\n",
    "        except:\n",
    "            somma += 0\n",
    "            \n",
    "    cosine_similarity = card_q * card_d_i * somma\n",
    "        \n",
    "    top_k_documents.append([round(cosine_similarity, 2), \"document_\"+str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq \n",
    "# using heapify to convert list into heap \n",
    "heapq.heapify(top_k_documents) \n",
    "show_top_k_documents = (heapq.nlargest(5, top_k_documents)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(columns=['bookTitle', 'Plot', 'Url', 'Similarity'])\n",
    "\n",
    "for row in show_top_k_documents:\n",
    "    i = int(row[1].split(\"_\")[1])\n",
    "    \n",
    "    #append row to the dataframe\n",
    "    new_row = {'bookTitle': df.loc[i, \"bookTitle\"], 'Plot': df1.loc[i, \"Plot\"], 'Url': df.loc[i, \"Url\"], 'Similarity': row[0]}\n",
    "    new_df = new_df.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where the Wild Things Are</td>\n",
       "      <td>Max, a wild and naughty boy, is sent to bed wi...</td>\n",
       "      <td>https://www.goodreads.com/book/show/19543.Wher...</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Picture of Dorian Gray</td>\n",
       "      <td>Written in his distinctively dazzling manner, ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/5297.The_P...</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Wrinkle in Time</td>\n",
       "      <td>It was a dark and stormy night.Out of this wil...</td>\n",
       "      <td>https://www.goodreads.com/book/show/33574273-a...</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>Could you survive on your own in the wild, wit...</td>\n",
       "      <td>https://www.goodreads.com/book/show/2767052-th...</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Lightning Thief</td>\n",
       "      <td>Alternate cover for this ISBN can be found her...</td>\n",
       "      <td>https://www.goodreads.com/book/show/28187.The_...</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    bookTitle  \\\n",
       "0   Where the Wild Things Are   \n",
       "1  The Picture of Dorian Gray   \n",
       "2           A Wrinkle in Time   \n",
       "3            The Hunger Games   \n",
       "4         The Lightning Thief   \n",
       "\n",
       "                                                Plot  \\\n",
       "0  Max, a wild and naughty boy, is sent to bed wi...   \n",
       "1  Written in his distinctively dazzling manner, ...   \n",
       "2  It was a dark and stormy night.Out of this wil...   \n",
       "3  Could you survive on your own in the wild, wit...   \n",
       "4  Alternate cover for this ISBN can be found her...   \n",
       "\n",
       "                                                 Url  Similarity  \n",
       "0  https://www.goodreads.com/book/show/19543.Wher...        0.11  \n",
       "1  https://www.goodreads.com/book/show/5297.The_P...        0.05  \n",
       "2  https://www.goodreads.com/book/show/33574273-a...        0.03  \n",
       "3  https://www.goodreads.com/book/show/2767052-th...        0.03  \n",
       "4  https://www.goodreads.com/book/show/28187.The_...        0.01  "
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

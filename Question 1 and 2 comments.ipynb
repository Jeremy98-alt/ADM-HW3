{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* check input and returns in functions\n",
    "* check that what i wrote is accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leonardo Masci, Jeremy Sapienza, Antonio Zappia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this homework is to create a search engine that returns some book suggestions, given an input from the user in the form of words. To achieve this, we first scraped the website Goodreads.com, in its \"best books ever\" section, to get a dataset of 30,000 books. Once we had our dataset, we created increasingly better search algorithms to get to a satisfying result. \n",
    "\n",
    "Finally, in the last exercise we faced a recursive algorithm and its running time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since no dataset was provided for this homework, we had to create our own dataset by scraping the given website, which was done in the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get the list of books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we had to get the list of all the books we wanted to add to our dataset. This included all the books found in the first 300 pages of the \"best books ever\" section.\n",
    "Therefore, we created a function that scrapes a certain number of pages from an initial url, to extract the url of each of them. The initial url and the following number of pages are given as input, while the resulting urls are stored in a txt file called \"urlpages.txt\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.get_urls(\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\", 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Crawl books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we had our txt file containing all the urls, we created a second function that downloads the html of each given url. These htmls are collected in different folders, depending on the corresponding page. Moreover, we added a try and except to be sure that all requests that we could lose will be re-scrape in a second moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.crawl_urls(300,\"urlpages.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after completing this step, we went on to actually parse the given html articles. We will obtain for each html article its article_i.tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.get_scraping(\"html_pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and concatening the final dataset for our purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since running the above mentioned functions took a long time, the three of us split the work among us. So in the end we run another function that returned our distinct outputs as tsv files and combined those three to create the final dataset we will be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.finaldataset_tsv(html_pages,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset, we can start working on the search engines we are interested in. We will start with a very simple one and work our way towards a more complex one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "df = pd.read_csv(\"finaloutput.tsv\", sep=\",\")\n",
    "df = df.reset_index(drop=True) # reset index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data and creating the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can procede with the search engines, we need to clean the dataset we have obtained and create a vocabulary. \n",
    "\n",
    "The clean data is needed to apply the search engine, as the input words may not match the ones in our file if processed raw. For this reason, we remove stopwords (such as 'a', 'the', etc.) and punctuation, which do not constitute valuable information about the text, and then we apply stemming. This way, we are left with only the root of words, which will make matching in the coming functions that much easier. These are all steps of the method Bag of Words, which has the ultimate goal of creating a vocabulary of all the unique words found in the dataframe.\n",
    "\n",
    "That is why we defined a copy of the original dataset, as the clean version of the dataset itself. This way we can create the needed vocabulary, which will be based on the 'Plot' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_copy = functions.cleaningDataset(df.copy()) # useful to obtain the vocabulary for each token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the dataset with apposite bag of words method, we create the vocabulary. The vocabulary is needed to create the inverted list as a dictionary and at the same time to map each word with its 'term_id_i'. We create this to avoid calculating the data structure every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = functions.createVocabulary(df_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid having to calculate it over and over, we save the vocabulary as csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.create_csv(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to receive the user's input, which is then processed the same way as the dataset was so that the matching will be possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query string: Survival games\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Insert your query string: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we obtain the clean query, which can be matched with the vocabulary tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['surviv', 'game']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanQString = functions.cleanQuery(query)\n",
    "cleanQString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why is this separated from the rest of vocabulary??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Term_id</th>\n",
       "      <th>Document_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>could</td>\n",
       "      <td>term_id_1</td>\n",
       "      <td>['document_0', 'document_26', 'document_38']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>surviv</td>\n",
       "      <td>term_id_2</td>\n",
       "      <td>['document_0', 'document_26', 'document_40', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wild</td>\n",
       "      <td>term_id_3</td>\n",
       "      <td>['document_0', 'document_8', 'document_19', 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>everi</td>\n",
       "      <td>term_id_4</td>\n",
       "      <td>['document_0', 'document_3', 'document_5', 'do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one</td>\n",
       "      <td>term_id_5</td>\n",
       "      <td>['document_0', 'document_9', 'document_14', 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>manheim</td>\n",
       "      <td>term_id_1989</td>\n",
       "      <td>['document_49']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>1996</td>\n",
       "      <td>term_id_1990</td>\n",
       "      <td>['document_49']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>access</td>\n",
       "      <td>term_id_1991</td>\n",
       "      <td>['document_49']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>poem</td>\n",
       "      <td>term_id_1992</td>\n",
       "      <td>['document_49']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>vers</td>\n",
       "      <td>term_id_1993</td>\n",
       "      <td>['document_49']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1993 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word       Term_id                                      Document_List\n",
       "0       could     term_id_1       ['document_0', 'document_26', 'document_38']\n",
       "1      surviv     term_id_2  ['document_0', 'document_26', 'document_40', '...\n",
       "2        wild     term_id_3  ['document_0', 'document_8', 'document_19', 'd...\n",
       "3       everi     term_id_4  ['document_0', 'document_3', 'document_5', 'do...\n",
       "4         one     term_id_5  ['document_0', 'document_9', 'document_14', 'd...\n",
       "...       ...           ...                                                ...\n",
       "1988  manheim  term_id_1989                                    ['document_49']\n",
       "1989     1996  term_id_1990                                    ['document_49']\n",
       "1990   access  term_id_1991                                    ['document_49']\n",
       "1991     poem  term_id_1992                                    ['document_49']\n",
       "1992     vers  term_id_1993                                    ['document_49']\n",
       "\n",
       "[1993 rows x 3 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = functions.open_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the inverted list based on that vocabulary. What is it? Why is it needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.inverted_list(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create the new search engine by using the clean query and the inverted list we have found in the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.search_engine1(cleanQString,'inv_lst.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"> YES, COMMENT IT... shows that is like the professor's request! </font>\n",
    "non c'è un cavolo da commentare se ancora non è runnata la cosa ahah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before defining the new search engine, we have to update our inverted list according to professors request, we want to have the pair list, for each term that we have into our vocabulary, the document where the term is present and the relative tf_idf score.\n",
    "\n",
    "We get the term frequency by simply counting the amount of times each term of the query appears in each document. Next we calculate the idf (inverse document frequency) as a way to give priority to those terms which are found less commonly in the document set. As the name suggests, this is an _inverse frequency_. Finally, we combine the two in the tf_idf, which is nothing more than their product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.new_inv_lst('inv_lst.csv',vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we get the user's input and we clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query string: could wild\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Insert your query string: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['could', 'wild']"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanQString = functions.cleanQuery(query)\n",
    "cleanQString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the second search engine, which is an improved version of the previous one as it takes into consideration not only the number of words in the query that are found in the plots, but also how often those words are mentioned. This makes the search engine more accurate in identifying books that are relevant given a certain query, which is in line with our final goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = functions.search_engine2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we illustrate the final dataset obtained with our second search engine. We chose to only see the top 5 suggestions, which are sorted by similarity score in the following table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where the Wild Things Are</td>\n",
       "      <td>Max, a wild and naughty boy, is sent to bed wi...</td>\n",
       "      <td>https://www.goodreads.com/book/show/19543.Wher...</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Picture of Dorian Gray</td>\n",
       "      <td>Written in his distinctively dazzling manner, ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/5297.The_P...</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Wrinkle in Time</td>\n",
       "      <td>It was a dark and stormy night.Out of this wil...</td>\n",
       "      <td>https://www.goodreads.com/book/show/33574273-a...</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>Could you survive on your own in the wild, wit...</td>\n",
       "      <td>https://www.goodreads.com/book/show/2767052-th...</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Lightning Thief</td>\n",
       "      <td>Alternate cover for this ISBN can be found her...</td>\n",
       "      <td>https://www.goodreads.com/book/show/28187.The_...</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    bookTitle  \\\n",
       "0   Where the Wild Things Are   \n",
       "1  The Picture of Dorian Gray   \n",
       "2           A Wrinkle in Time   \n",
       "3            The Hunger Games   \n",
       "4         The Lightning Thief   \n",
       "\n",
       "                                                Plot  \\\n",
       "0  Max, a wild and naughty boy, is sent to bed wi...   \n",
       "1  Written in his distinctively dazzling manner, ...   \n",
       "2  It was a dark and stormy night.Out of this wil...   \n",
       "3  Could you survive on your own in the wild, wit...   \n",
       "4  Alternate cover for this ISBN can be found her...   \n",
       "\n",
       "                                                 Url  Similarity  \n",
       "0  https://www.goodreads.com/book/show/19543.Wher...        0.11  \n",
       "1  https://www.goodreads.com/book/show/5297.The_P...        0.05  \n",
       "2  https://www.goodreads.com/book/show/33574273-a...        0.03  \n",
       "3  https://www.goodreads.com/book/show/2767052-th...        0.03  \n",
       "4  https://www.goodreads.com/book/show/28187.The_...        0.01  "
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Define a new score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to improve again on our search engine, this time by taking into account multiple variables. \n",
    "* similarity score also in book title and author (year it was published, series, characters and setting)\n",
    "* we could define a \"popularity\" score that takes into account the average rating and the number of reviews\n",
    "* we then could combine the two to create the final score and sorting order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
